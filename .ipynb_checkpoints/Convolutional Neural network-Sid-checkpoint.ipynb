{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import pdb\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_data = pd.read_csv('fashion-mnist_train.csv')\n",
    "    test_data = pd.read_csv('fashion-mnist_test.csv')\n",
    "    x_train = train_data[train_data.columns[1:]].values\n",
    "    y_train = train_data.label.values\n",
    "    x_test = test_data[test_data.columns[1:]].values\n",
    "    y_test = test_data.label.values\n",
    "    return map(tensor, (x_train, y_train, x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_data()\n",
    "train_n, train_m = x_train.shape\n",
    "test_n, test_m = x_test.shape\n",
    "n_cls = y_train.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129ec5fd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[torch.randint(train_n, (1,))].view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convutional_model(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(convutional_model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5) \n",
    "        self.pool = nn.AvgPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3) \n",
    "        self.conv3 = nn.Conv2d(16, 32, 3)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3)\n",
    "        self.fc1 = nn.Linear(32*3*3 , 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x))) \n",
    "        x = self.pool(F.relu(self.conv3(x))) \n",
    "        x = self.pool(F.relu(self.conv4(x))) \n",
    "        x = x.view(-1, 16*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMnistNet(nn.Module):\n",
    "    # Based on Lecunn's Lenet architecture\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1, 5, 5)\n",
      "\n",
      "(8,)\n",
      "\n",
      "(16, 8, 3, 3)\n",
      "\n",
      "(16,)\n",
      "\n",
      "(32, 16, 3, 3)\n",
      "\n",
      "(32,)\n",
      "\n",
      "(32, 32, 3, 3)\n",
      "\n",
      "(32,)\n",
      "\n",
      "(120, 288)\n",
      "\n",
      "(120,)\n",
      "\n",
      "(84, 120)\n",
      "\n",
      "(84,)\n",
      "\n",
      "(10, 84)\n",
      "\n",
      "(10,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = convutional_model()\n",
    "for param in model.parameters():\n",
    "    print(str(param.data.numpy().shape)+'\\n')\n",
    "    costfun = nn.CrossEntropyLoss()\n",
    "    opm = torch.optim.SGD(model.parameters(),lr =0.5, momentum =0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "model = FashionMnistNet() # Creating a model\n",
    "lr = 0.05 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 32 # batch size \n",
    "loss_func = F.cross_entropy # loss function \n",
    "opt = optim.SGD(model.parameters(), lr=lr) # optimizer\n",
    "accuracy_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model:  84.37999725341797\n",
      "Accuracy of Model:  86.97000122070312\n",
      "Accuracy of Model:  88.5999984741211\n",
      "Accuracy of Model:  88.83000183105469\n",
      "Accuracy of Model:  89.33000183105469\n",
      "Accuracy of Model:  89.44999694824219\n",
      "Accuracy of Model:  89.7300033569336\n",
      "Accuracy of Model:  89.86000061035156\n",
      "Accuracy of Model:  89.94000244140625\n",
      "Accuracy of Model:  89.98999786376953\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].float().reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model.forward(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model.eval()\n",
    "    #print(model.training)\n",
    "    with torch.no_grad():\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].float().reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy of Model: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals.append((accuracy*100/test_n).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        clas = 0\n",
    "        for i,batch in range(testset,0):\n",
    "            data, output = batch\n",
    "            prediction = model(data)\n",
    "            loss = costFunction(predcition,output)\n",
    "            clas += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i%100 ==0:\n",
    "                losses.append(append.items())\n",
    "        \n",
    "            if i%1000 ==0:\n",
    "                print('[%d %d] Loss is : %.4f'% (epoch+1,i+1,clas/1000))\n",
    "                clas=0\n",
    "                accuracy()\n",
    "                plt.plot(losses, label='epoch'+str(epoch))\n",
    "                plt.legend(loc=1,mode='expanded', shadow = True,ncol=2)\n",
    "    plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy():\n",
    "    model.eval()\n",
    "    corrhit= 0\n",
    "    total = 0\n",
    "    accuracy = 0\n",
    "    for batches in x_test:\n",
    "        data, output = batches\n",
    "        prediction = model(data)\n",
    "        _, prediction = torch.max(prediction.data,1)\n",
    "        total += output.size(0)\n",
    "        corrhit +=(prediction == output).sum().item()\n",
    "        accuracy = (corrhit/total)*100\n",
    "        \n",
    "    print('Accuracy of model = '+str(accuracy)) \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127e83e48>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(72.9505), tensor(89.9669))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ### Normalization\n",
    "x_train, x_test = x_train.float(), x_test.float()\n",
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test = normalize(x_test, train_mean, train_std) # note this normalize test data also with training mean and standard deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  83.79000091552734\n",
      "Accuracy:  86.86000061035156\n",
      "Accuracy:  87.94999694824219\n",
      "Accuracy:  88.51000213623047\n",
      "Accuracy:  89.02999877929688\n",
      "Accuracy:  89.33000183105469\n",
      "Accuracy:  89.36000061035156\n",
      "Accuracy:  89.7300033569336\n",
      "Accuracy:  89.91000366210938\n",
      "Accuracy:  89.98999786376953\n"
     ]
    }
   ],
   "source": [
    "model_wnd = FashionMnistNet()\n",
    "lr = 0.05 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model_wnd.parameters(), lr=lr)\n",
    "accuracy_vals_wnd = []\n",
    "for epoch in range(epochs):\n",
    "    model_wnd.train()\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model_wnd.forward(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model_wnd.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        validation_size = int(test_n/10)\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model_wnd.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals_wnd.append((accuracy*100/test_n).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x127eba780>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_vals_wnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(net, loss_func, init_value = 1e-8, final_value=10., beta = 0.98, bs = 32):\n",
    "    num = (train_n-1)//bs + 1 # num of batches \n",
    "    mult = (final_value/init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0.\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        batch_num += 1\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(xb)\n",
    "        loss = loss_func(outputs, yb)\n",
    "        #Compute the smoothed loss\n",
    "        print(\"loss: \", loss.item())\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            return log_lrs, losses\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        #Do the SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    return log_lrs, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3227152824401855\n",
      "loss:  2.311708688735962\n",
      "loss:  2.3240621089935303\n",
      "loss:  2.287216901779175\n",
      "loss:  2.288522720336914\n",
      "loss:  2.3075733184814453\n",
      "loss:  2.2757925987243652\n",
      "loss:  2.3218305110931396\n",
      "loss:  2.3013381958007812\n",
      "loss:  2.3093748092651367\n",
      "loss:  2.325207233428955\n",
      "loss:  2.3111743927001953\n",
      "loss:  2.284435749053955\n",
      "loss:  2.295259714126587\n",
      "loss:  2.277996778488159\n",
      "loss:  2.301677942276001\n",
      "loss:  2.3211724758148193\n",
      "loss:  2.320497512817383\n",
      "loss:  2.2928574085235596\n",
      "loss:  2.288891553878784\n",
      "loss:  2.3372888565063477\n",
      "loss:  2.3041694164276123\n",
      "loss:  2.3168587684631348\n",
      "loss:  2.3237051963806152\n",
      "loss:  2.3097827434539795\n",
      "loss:  2.3124301433563232\n",
      "loss:  2.295287847518921\n",
      "loss:  2.3147788047790527\n",
      "loss:  2.3023886680603027\n",
      "loss:  2.320051908493042\n",
      "loss:  2.3275671005249023\n",
      "loss:  2.3110768795013428\n",
      "loss:  2.3028836250305176\n",
      "loss:  2.3036694526672363\n",
      "loss:  2.2994384765625\n",
      "loss:  2.3081982135772705\n",
      "loss:  2.321458578109741\n",
      "loss:  2.3059194087982178\n",
      "loss:  2.306075096130371\n",
      "loss:  2.2986414432525635\n",
      "loss:  2.311556339263916\n",
      "loss:  2.3298535346984863\n",
      "loss:  2.3259031772613525\n",
      "loss:  2.314432382583618\n",
      "loss:  2.2845051288604736\n",
      "loss:  2.2998762130737305\n",
      "loss:  2.305302858352661\n",
      "loss:  2.3230063915252686\n",
      "loss:  2.326747179031372\n",
      "loss:  2.313375949859619\n",
      "loss:  2.307913303375244\n",
      "loss:  2.3122384548187256\n",
      "loss:  2.3044066429138184\n",
      "loss:  2.3384883403778076\n",
      "loss:  2.3128015995025635\n",
      "loss:  2.340160608291626\n",
      "loss:  2.3003792762756348\n",
      "loss:  2.2962350845336914\n",
      "loss:  2.3124659061431885\n",
      "loss:  2.2761764526367188\n",
      "loss:  2.297741174697876\n",
      "loss:  2.3112237453460693\n",
      "loss:  2.322031021118164\n",
      "loss:  2.2758378982543945\n",
      "loss:  2.2903943061828613\n",
      "loss:  2.3200809955596924\n",
      "loss:  2.290532112121582\n",
      "loss:  2.2948524951934814\n",
      "loss:  2.30448842048645\n",
      "loss:  2.3152387142181396\n",
      "loss:  2.321826696395874\n",
      "loss:  2.292250394821167\n",
      "loss:  2.2843706607818604\n",
      "loss:  2.2857489585876465\n",
      "loss:  2.3068735599517822\n",
      "loss:  2.306583881378174\n",
      "loss:  2.317514657974243\n",
      "loss:  2.3105430603027344\n",
      "loss:  2.306417226791382\n",
      "loss:  2.2914679050445557\n",
      "loss:  2.3005261421203613\n",
      "loss:  2.3061351776123047\n",
      "loss:  2.32635235786438\n",
      "loss:  2.3147895336151123\n",
      "loss:  2.2904324531555176\n",
      "loss:  2.308607578277588\n",
      "loss:  2.291011333465576\n",
      "loss:  2.311638355255127\n",
      "loss:  2.3268332481384277\n",
      "loss:  2.299247980117798\n",
      "loss:  2.3136870861053467\n",
      "loss:  2.2943594455718994\n",
      "loss:  2.331332206726074\n",
      "loss:  2.2988123893737793\n",
      "loss:  2.2896132469177246\n",
      "loss:  2.3133652210235596\n",
      "loss:  2.305391311645508\n",
      "loss:  2.314586877822876\n",
      "loss:  2.336721897125244\n",
      "loss:  2.289842128753662\n",
      "loss:  2.2993030548095703\n",
      "loss:  2.3121016025543213\n",
      "loss:  2.2884159088134766\n",
      "loss:  2.297645330429077\n",
      "loss:  2.3131275177001953\n",
      "loss:  2.3076882362365723\n",
      "loss:  2.3071811199188232\n",
      "loss:  2.3037071228027344\n",
      "loss:  2.2985172271728516\n",
      "loss:  2.318995714187622\n",
      "loss:  2.329798698425293\n",
      "loss:  2.3104634284973145\n",
      "loss:  2.299987316131592\n",
      "loss:  2.2985599040985107\n",
      "loss:  2.3294320106506348\n",
      "loss:  2.31119704246521\n",
      "loss:  2.295767068862915\n",
      "loss:  2.3148961067199707\n",
      "loss:  2.299027919769287\n",
      "loss:  2.258572816848755\n",
      "loss:  2.298704147338867\n",
      "loss:  2.3159196376800537\n",
      "loss:  2.3047983646392822\n",
      "loss:  2.27691650390625\n",
      "loss:  2.304673910140991\n",
      "loss:  2.308717966079712\n",
      "loss:  2.3030025959014893\n",
      "loss:  2.3033177852630615\n",
      "loss:  2.294781446456909\n",
      "loss:  2.306086301803589\n",
      "loss:  2.310849189758301\n",
      "loss:  2.3295905590057373\n",
      "loss:  2.3240835666656494\n",
      "loss:  2.3084685802459717\n",
      "loss:  2.3062493801116943\n",
      "loss:  2.3390824794769287\n",
      "loss:  2.296934127807617\n",
      "loss:  2.3261427879333496\n",
      "loss:  2.320244789123535\n",
      "loss:  2.314966917037964\n",
      "loss:  2.294651985168457\n",
      "loss:  2.306746244430542\n",
      "loss:  2.2912864685058594\n",
      "loss:  2.3177733421325684\n",
      "loss:  2.286728858947754\n",
      "loss:  2.2878308296203613\n",
      "loss:  2.311152696609497\n",
      "loss:  2.304582118988037\n",
      "loss:  2.2986133098602295\n",
      "loss:  2.3174333572387695\n",
      "loss:  2.287368059158325\n",
      "loss:  2.284280300140381\n",
      "loss:  2.315380573272705\n",
      "loss:  2.30483341217041\n",
      "loss:  2.32419753074646\n",
      "loss:  2.3385965824127197\n",
      "loss:  2.2993977069854736\n",
      "loss:  2.290151596069336\n",
      "loss:  2.3074185848236084\n",
      "loss:  2.324441432952881\n",
      "loss:  2.3060801029205322\n",
      "loss:  2.3164961338043213\n",
      "loss:  2.3076820373535156\n",
      "loss:  2.293950080871582\n",
      "loss:  2.2978668212890625\n",
      "loss:  2.301821708679199\n",
      "loss:  2.301510810852051\n",
      "loss:  2.3086185455322266\n",
      "loss:  2.31506609916687\n",
      "loss:  2.3019559383392334\n",
      "loss:  2.321704149246216\n",
      "loss:  2.2857675552368164\n",
      "loss:  2.3011927604675293\n",
      "loss:  2.307039737701416\n",
      "loss:  2.277284622192383\n",
      "loss:  2.3169105052948\n",
      "loss:  2.3126425743103027\n",
      "loss:  2.3230087757110596\n",
      "loss:  2.305968761444092\n",
      "loss:  2.305948495864868\n",
      "loss:  2.32521653175354\n",
      "loss:  2.3044497966766357\n",
      "loss:  2.3009209632873535\n",
      "loss:  2.2796754837036133\n",
      "loss:  2.2746798992156982\n",
      "loss:  2.2948403358459473\n",
      "loss:  2.302481174468994\n",
      "loss:  2.320512056350708\n",
      "loss:  2.3158187866210938\n",
      "loss:  2.303267478942871\n",
      "loss:  2.3108949661254883\n",
      "loss:  2.28842830657959\n",
      "loss:  2.3106327056884766\n",
      "loss:  2.299395799636841\n",
      "loss:  2.291384696960449\n",
      "loss:  2.3274829387664795\n",
      "loss:  2.3090124130249023\n",
      "loss:  2.3149356842041016\n",
      "loss:  2.2903854846954346\n",
      "loss:  2.294053316116333\n",
      "loss:  2.3051364421844482\n",
      "loss:  2.314974308013916\n",
      "loss:  2.2854068279266357\n",
      "loss:  2.2852768898010254\n",
      "loss:  2.29854416847229\n",
      "loss:  2.314347743988037\n",
      "loss:  2.3073012828826904\n",
      "loss:  2.27148175239563\n",
      "loss:  2.335439682006836\n",
      "loss:  2.303344964981079\n",
      "loss:  2.3373420238494873\n",
      "loss:  2.326620578765869\n",
      "loss:  2.3135151863098145\n",
      "loss:  2.3052592277526855\n",
      "loss:  2.303968667984009\n",
      "loss:  2.2939257621765137\n",
      "loss:  2.311108112335205\n",
      "loss:  2.289285659790039\n",
      "loss:  2.3130791187286377\n",
      "loss:  2.3171749114990234\n",
      "loss:  2.2939305305480957\n",
      "loss:  2.336228370666504\n",
      "loss:  2.2817745208740234\n",
      "loss:  2.3027799129486084\n",
      "loss:  2.298307418823242\n",
      "loss:  2.3002982139587402\n",
      "loss:  2.293290376663208\n",
      "loss:  2.3116888999938965\n",
      "loss:  2.2790255546569824\n",
      "loss:  2.3151586055755615\n",
      "loss:  2.3271262645721436\n",
      "loss:  2.2943384647369385\n",
      "loss:  2.2786359786987305\n",
      "loss:  2.2861175537109375\n",
      "loss:  2.292332887649536\n",
      "loss:  2.2961575984954834\n",
      "loss:  2.2942006587982178\n",
      "loss:  2.3200809955596924\n",
      "loss:  2.308651924133301\n",
      "loss:  2.306572675704956\n",
      "loss:  2.3082668781280518\n",
      "loss:  2.3228867053985596\n",
      "loss:  2.301612138748169\n",
      "loss:  2.310176372528076\n",
      "loss:  2.281663179397583\n",
      "loss:  2.3186776638031006\n",
      "loss:  2.331062078475952\n",
      "loss:  2.3033385276794434\n",
      "loss:  2.2935919761657715\n",
      "loss:  2.292203903198242\n",
      "loss:  2.3074426651000977\n",
      "loss:  2.3117499351501465\n",
      "loss:  2.3295342922210693\n",
      "loss:  2.3181545734405518\n",
      "loss:  2.295175313949585\n",
      "loss:  2.322636842727661\n",
      "loss:  2.3185245990753174\n",
      "loss:  2.288867473602295\n",
      "loss:  2.3132081031799316\n",
      "loss:  2.3085453510284424\n",
      "loss:  2.316490650177002\n",
      "loss:  2.297294855117798\n",
      "loss:  2.2927918434143066\n",
      "loss:  2.271942138671875\n",
      "loss:  2.3056700229644775\n",
      "loss:  2.2855746746063232\n",
      "loss:  2.3020365238189697\n",
      "loss:  2.3041632175445557\n",
      "loss:  2.327597141265869\n",
      "loss:  2.3011832237243652\n",
      "loss:  2.331017255783081\n",
      "loss:  2.3119587898254395\n",
      "loss:  2.323946475982666\n",
      "loss:  2.3132097721099854\n",
      "loss:  2.272911310195923\n",
      "loss:  2.2625980377197266\n",
      "loss:  2.299720525741577\n",
      "loss:  2.305833578109741\n",
      "loss:  2.2943294048309326\n",
      "loss:  2.3183085918426514\n",
      "loss:  2.3181769847869873\n",
      "loss:  2.320575475692749\n",
      "loss:  2.3065695762634277\n",
      "loss:  2.3142640590667725\n",
      "loss:  2.3169469833374023\n",
      "loss:  2.3155405521392822\n",
      "loss:  2.2807118892669678\n",
      "loss:  2.332677125930786\n",
      "loss:  2.2830772399902344\n",
      "loss:  2.3013360500335693\n",
      "loss:  2.3015201091766357\n",
      "loss:  2.3230669498443604\n",
      "loss:  2.269299268722534\n",
      "loss:  2.298208475112915\n",
      "loss:  2.285737991333008\n",
      "loss:  2.293539524078369\n",
      "loss:  2.3041837215423584\n",
      "loss:  2.2936055660247803\n",
      "loss:  2.315089225769043\n",
      "loss:  2.3013851642608643\n",
      "loss:  2.3263044357299805\n",
      "loss:  2.2908971309661865\n",
      "loss:  2.306541681289673\n",
      "loss:  2.3010902404785156\n",
      "loss:  2.3263442516326904\n",
      "loss:  2.3157691955566406\n",
      "loss:  2.2883598804473877\n",
      "loss:  2.286763906478882\n",
      "loss:  2.330793857574463\n",
      "loss:  2.2971413135528564\n",
      "loss:  2.3267674446105957\n",
      "loss:  2.2907514572143555\n",
      "loss:  2.3190739154815674\n",
      "loss:  2.314892053604126\n",
      "loss:  2.302898645401001\n",
      "loss:  2.321186065673828\n",
      "loss:  2.293248176574707\n",
      "loss:  2.3322038650512695\n",
      "loss:  2.3203792572021484\n",
      "loss:  2.2914130687713623\n",
      "loss:  2.3212890625\n",
      "loss:  2.3210363388061523\n",
      "loss:  2.2903075218200684\n",
      "loss:  2.290382146835327\n",
      "loss:  2.308004379272461\n",
      "loss:  2.313580274581909\n",
      "loss:  2.307188034057617\n",
      "loss:  2.3101935386657715\n",
      "loss:  2.32633376121521\n",
      "loss:  2.2990224361419678\n",
      "loss:  2.286971092224121\n",
      "loss:  2.3020689487457275\n",
      "loss:  2.302366256713867\n",
      "loss:  2.3017351627349854\n",
      "loss:  2.2844173908233643\n",
      "loss:  2.309297800064087\n",
      "loss:  2.312318801879883\n",
      "loss:  2.2896721363067627\n",
      "loss:  2.277477741241455\n",
      "loss:  2.3200137615203857\n",
      "loss:  2.3043670654296875\n",
      "loss:  2.324859619140625\n",
      "loss:  2.2936620712280273\n",
      "loss:  2.308769464492798\n",
      "loss:  2.318415403366089\n",
      "loss:  2.3212897777557373\n",
      "loss:  2.3034164905548096\n",
      "loss:  2.3277924060821533\n",
      "loss:  2.3259198665618896\n",
      "loss:  2.2813520431518555\n",
      "loss:  2.3033483028411865\n",
      "loss:  2.281144142150879\n",
      "loss:  2.292492151260376\n",
      "loss:  2.3060574531555176\n",
      "loss:  2.309321403503418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3317525386810303\n",
      "loss:  2.3044955730438232\n",
      "loss:  2.304144859313965\n",
      "loss:  2.3080451488494873\n",
      "loss:  2.2987160682678223\n",
      "loss:  2.2922251224517822\n",
      "loss:  2.307677984237671\n",
      "loss:  2.3073501586914062\n",
      "loss:  2.3026230335235596\n",
      "loss:  2.338778018951416\n",
      "loss:  2.2803361415863037\n",
      "loss:  2.3042821884155273\n",
      "loss:  2.307725191116333\n",
      "loss:  2.3111138343811035\n",
      "loss:  2.299677848815918\n",
      "loss:  2.321505308151245\n",
      "loss:  2.3162240982055664\n",
      "loss:  2.3097293376922607\n",
      "loss:  2.30785870552063\n",
      "loss:  2.315976619720459\n",
      "loss:  2.3242692947387695\n",
      "loss:  2.3087689876556396\n",
      "loss:  2.3378233909606934\n",
      "loss:  2.3048934936523438\n",
      "loss:  2.296937942504883\n",
      "loss:  2.3109793663024902\n",
      "loss:  2.3172078132629395\n",
      "loss:  2.2826032638549805\n",
      "loss:  2.3027377128601074\n",
      "loss:  2.3078949451446533\n",
      "loss:  2.319491386413574\n",
      "loss:  2.267423152923584\n",
      "loss:  2.2942192554473877\n",
      "loss:  2.2838821411132812\n",
      "loss:  2.299081325531006\n",
      "loss:  2.2934606075286865\n",
      "loss:  2.3014132976531982\n",
      "loss:  2.2637739181518555\n",
      "loss:  2.3198440074920654\n",
      "loss:  2.3017446994781494\n",
      "loss:  2.314194917678833\n",
      "loss:  2.303575277328491\n",
      "loss:  2.299555540084839\n",
      "loss:  2.301530122756958\n",
      "loss:  2.2995545864105225\n",
      "loss:  2.3360719680786133\n",
      "loss:  2.30501389503479\n",
      "loss:  2.304021120071411\n",
      "loss:  2.331789493560791\n",
      "loss:  2.2930731773376465\n",
      "loss:  2.288771390914917\n",
      "loss:  2.303410530090332\n",
      "loss:  2.293132781982422\n",
      "loss:  2.2985641956329346\n",
      "loss:  2.283113479614258\n",
      "loss:  2.3045132160186768\n",
      "loss:  2.313563585281372\n",
      "loss:  2.30083966255188\n",
      "loss:  2.3326172828674316\n",
      "loss:  2.326587438583374\n",
      "loss:  2.303605794906616\n",
      "loss:  2.3361711502075195\n",
      "loss:  2.2885775566101074\n",
      "loss:  2.2918708324432373\n",
      "loss:  2.306056499481201\n",
      "loss:  2.3055124282836914\n",
      "loss:  2.3156449794769287\n",
      "loss:  2.318929672241211\n",
      "loss:  2.294748067855835\n",
      "loss:  2.314225435256958\n",
      "loss:  2.330972671508789\n",
      "loss:  2.2838213443756104\n",
      "loss:  2.292342185974121\n",
      "loss:  2.3026318550109863\n",
      "loss:  2.304781436920166\n",
      "loss:  2.314668655395508\n",
      "loss:  2.306427478790283\n",
      "loss:  2.305879592895508\n",
      "loss:  2.3423361778259277\n",
      "loss:  2.308018207550049\n",
      "loss:  2.327211856842041\n",
      "loss:  2.301696538925171\n",
      "loss:  2.295377016067505\n",
      "loss:  2.3210883140563965\n",
      "loss:  2.2931034564971924\n",
      "loss:  2.308919668197632\n",
      "loss:  2.283600091934204\n",
      "loss:  2.3030431270599365\n",
      "loss:  2.3188536167144775\n",
      "loss:  2.300187349319458\n",
      "loss:  2.3399105072021484\n",
      "loss:  2.31766939163208\n",
      "loss:  2.3148415088653564\n",
      "loss:  2.2935163974761963\n",
      "loss:  2.2940359115600586\n",
      "loss:  2.3164737224578857\n",
      "loss:  2.346182346343994\n",
      "loss:  2.2960948944091797\n",
      "loss:  2.306575298309326\n",
      "loss:  2.325014114379883\n",
      "loss:  2.3192086219787598\n",
      "loss:  2.3441152572631836\n",
      "loss:  2.311180353164673\n",
      "loss:  2.314829111099243\n",
      "loss:  2.327061653137207\n",
      "loss:  2.3249804973602295\n",
      "loss:  2.325249195098877\n",
      "loss:  2.3203625679016113\n",
      "loss:  2.326110601425171\n",
      "loss:  2.3267245292663574\n",
      "loss:  2.2794880867004395\n",
      "loss:  2.292149782180786\n",
      "loss:  2.3074233531951904\n",
      "loss:  2.349924087524414\n",
      "loss:  2.307851791381836\n",
      "loss:  2.327634572982788\n",
      "loss:  2.311305522918701\n",
      "loss:  2.2859668731689453\n",
      "loss:  2.2919580936431885\n",
      "loss:  2.31876540184021\n",
      "loss:  2.2976043224334717\n",
      "loss:  2.316145181655884\n",
      "loss:  2.297565221786499\n",
      "loss:  2.3050246238708496\n",
      "loss:  2.3032634258270264\n",
      "loss:  2.309293270111084\n",
      "loss:  2.328949213027954\n",
      "loss:  2.3166158199310303\n",
      "loss:  2.306788444519043\n",
      "loss:  2.3266425132751465\n",
      "loss:  2.3156189918518066\n",
      "loss:  2.307311773300171\n",
      "loss:  2.3203203678131104\n",
      "loss:  2.324686288833618\n",
      "loss:  2.3200409412384033\n",
      "loss:  2.2858426570892334\n",
      "loss:  2.3025383949279785\n",
      "loss:  2.3118669986724854\n",
      "loss:  2.2946059703826904\n",
      "loss:  2.3216712474823\n",
      "loss:  2.2993526458740234\n",
      "loss:  2.275407552719116\n",
      "loss:  2.289721727371216\n",
      "loss:  2.2904372215270996\n",
      "loss:  2.2996857166290283\n",
      "loss:  2.3104400634765625\n",
      "loss:  2.30702543258667\n",
      "loss:  2.2978169918060303\n",
      "loss:  2.320054531097412\n",
      "loss:  2.3012068271636963\n",
      "loss:  2.314814805984497\n",
      "loss:  2.320577383041382\n",
      "loss:  2.3154571056365967\n",
      "loss:  2.296637773513794\n",
      "loss:  2.3139021396636963\n",
      "loss:  2.3154537677764893\n",
      "loss:  2.3154261112213135\n",
      "loss:  2.301255226135254\n",
      "loss:  2.325793743133545\n",
      "loss:  2.3143999576568604\n",
      "loss:  2.303833246231079\n",
      "loss:  2.2993178367614746\n",
      "loss:  2.3150832653045654\n",
      "loss:  2.310041666030884\n",
      "loss:  2.316405773162842\n",
      "loss:  2.286118745803833\n",
      "loss:  2.3091814517974854\n",
      "loss:  2.3029191493988037\n",
      "loss:  2.3283963203430176\n",
      "loss:  2.3226001262664795\n",
      "loss:  2.3182642459869385\n",
      "loss:  2.289235830307007\n",
      "loss:  2.3040571212768555\n",
      "loss:  2.3053271770477295\n",
      "loss:  2.324620246887207\n",
      "loss:  2.2851028442382812\n",
      "loss:  2.307694673538208\n",
      "loss:  2.300748825073242\n",
      "loss:  2.298553705215454\n",
      "loss:  2.2891769409179688\n",
      "loss:  2.308154582977295\n",
      "loss:  2.3167226314544678\n",
      "loss:  2.2943620681762695\n",
      "loss:  2.3522562980651855\n",
      "loss:  2.3070156574249268\n",
      "loss:  2.3456227779388428\n",
      "loss:  2.297349691390991\n",
      "loss:  2.3168444633483887\n",
      "loss:  2.3083336353302\n",
      "loss:  2.29677677154541\n",
      "loss:  2.317164421081543\n",
      "loss:  2.3422317504882812\n",
      "loss:  2.296976089477539\n",
      "loss:  2.284237861633301\n",
      "loss:  2.3109331130981445\n",
      "loss:  2.2962706089019775\n",
      "loss:  2.299022912979126\n",
      "loss:  2.318847894668579\n",
      "loss:  2.2941782474517822\n",
      "loss:  2.305211305618286\n",
      "loss:  2.311119556427002\n",
      "loss:  2.2978005409240723\n",
      "loss:  2.333634614944458\n",
      "loss:  2.284367322921753\n",
      "loss:  2.353041887283325\n",
      "loss:  2.2875607013702393\n",
      "loss:  2.3070714473724365\n",
      "loss:  2.3109612464904785\n",
      "loss:  2.303826332092285\n",
      "loss:  2.312007427215576\n",
      "loss:  2.3049356937408447\n",
      "loss:  2.3126413822174072\n",
      "loss:  2.316450595855713\n",
      "loss:  2.2854533195495605\n",
      "loss:  2.3071091175079346\n",
      "loss:  2.3300681114196777\n",
      "loss:  2.299107789993286\n",
      "loss:  2.3023617267608643\n",
      "loss:  2.3203158378601074\n",
      "loss:  2.3187835216522217\n",
      "loss:  2.292290210723877\n",
      "loss:  2.295320749282837\n",
      "loss:  2.3076367378234863\n",
      "loss:  2.323366403579712\n",
      "loss:  2.298067808151245\n",
      "loss:  2.2804954051971436\n",
      "loss:  2.2867660522460938\n",
      "loss:  2.3142600059509277\n",
      "loss:  2.2985875606536865\n",
      "loss:  2.303739547729492\n",
      "loss:  2.302464723587036\n",
      "loss:  2.333709239959717\n",
      "loss:  2.280427932739258\n",
      "loss:  2.3024513721466064\n",
      "loss:  2.3242547512054443\n",
      "loss:  2.298469066619873\n",
      "loss:  2.299105644226074\n",
      "loss:  2.3182287216186523\n",
      "loss:  2.316925048828125\n",
      "loss:  2.2870655059814453\n",
      "loss:  2.3153560161590576\n",
      "loss:  2.31966233253479\n",
      "loss:  2.2921462059020996\n",
      "loss:  2.2974884510040283\n",
      "loss:  2.332709312438965\n",
      "loss:  2.336275815963745\n",
      "loss:  2.324352264404297\n",
      "loss:  2.293586492538452\n",
      "loss:  2.2873435020446777\n",
      "loss:  2.2923941612243652\n",
      "loss:  2.303035020828247\n",
      "loss:  2.3216745853424072\n",
      "loss:  2.3034026622772217\n",
      "loss:  2.352452278137207\n",
      "loss:  2.2984397411346436\n",
      "loss:  2.3211050033569336\n",
      "loss:  2.293365955352783\n",
      "loss:  2.325733184814453\n",
      "loss:  2.3031513690948486\n",
      "loss:  2.2949767112731934\n",
      "loss:  2.313202142715454\n",
      "loss:  2.3101859092712402\n",
      "loss:  2.3344013690948486\n",
      "loss:  2.31807279586792\n",
      "loss:  2.310853958129883\n",
      "loss:  2.2992031574249268\n",
      "loss:  2.310089111328125\n",
      "loss:  2.3188564777374268\n",
      "loss:  2.3245279788970947\n",
      "loss:  2.3061811923980713\n",
      "loss:  2.2838094234466553\n",
      "loss:  2.28263521194458\n",
      "loss:  2.3041605949401855\n",
      "loss:  2.325025796890259\n",
      "loss:  2.3170158863067627\n",
      "loss:  2.3097400665283203\n",
      "loss:  2.288844347000122\n",
      "loss:  2.3121304512023926\n",
      "loss:  2.303814172744751\n",
      "loss:  2.2885918617248535\n",
      "loss:  2.314695358276367\n",
      "loss:  2.3000895977020264\n",
      "loss:  2.316143751144409\n",
      "loss:  2.274003744125366\n",
      "loss:  2.3256845474243164\n",
      "loss:  2.3115718364715576\n",
      "loss:  2.301604747772217\n",
      "loss:  2.3399693965911865\n",
      "loss:  2.293708562850952\n",
      "loss:  2.3046369552612305\n",
      "loss:  2.3150410652160645\n",
      "loss:  2.3127710819244385\n",
      "loss:  2.2961339950561523\n",
      "loss:  2.3104653358459473\n",
      "loss:  2.330268383026123\n",
      "loss:  2.30454421043396\n",
      "loss:  2.308077812194824\n",
      "loss:  2.3142449855804443\n",
      "loss:  2.315314531326294\n",
      "loss:  2.303562641143799\n",
      "loss:  2.292335033416748\n",
      "loss:  2.3258705139160156\n",
      "loss:  2.3232812881469727\n",
      "loss:  2.330223560333252\n",
      "loss:  2.3157145977020264\n",
      "loss:  2.284008502960205\n",
      "loss:  2.302996873855591\n",
      "loss:  2.2940738201141357\n",
      "loss:  2.3030776977539062\n",
      "loss:  2.3008341789245605\n",
      "loss:  2.323765516281128\n",
      "loss:  2.293250560760498\n",
      "loss:  2.2917559146881104\n",
      "loss:  2.3204195499420166\n",
      "loss:  2.297356128692627\n",
      "loss:  2.289916753768921\n",
      "loss:  2.3090038299560547\n",
      "loss:  2.2809011936187744\n",
      "loss:  2.3066468238830566\n",
      "loss:  2.319293737411499\n",
      "loss:  2.281369686126709\n",
      "loss:  2.3075051307678223\n",
      "loss:  2.2912745475769043\n",
      "loss:  2.3170435428619385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.311321258544922\n",
      "loss:  2.306612014770508\n",
      "loss:  2.317589282989502\n",
      "loss:  2.283557176589966\n",
      "loss:  2.2900562286376953\n",
      "loss:  2.3268895149230957\n",
      "loss:  2.278920888900757\n",
      "loss:  2.310887575149536\n",
      "loss:  2.3049862384796143\n",
      "loss:  2.2992496490478516\n",
      "loss:  2.3277621269226074\n",
      "loss:  2.320408821105957\n",
      "loss:  2.301701068878174\n",
      "loss:  2.31774640083313\n",
      "loss:  2.3056342601776123\n",
      "loss:  2.3183774948120117\n",
      "loss:  2.305551528930664\n",
      "loss:  2.325925588607788\n",
      "loss:  2.3147778511047363\n",
      "loss:  2.31467604637146\n",
      "loss:  2.3055906295776367\n",
      "loss:  2.3165340423583984\n",
      "loss:  2.308785915374756\n",
      "loss:  2.277761936187744\n",
      "loss:  2.261432647705078\n",
      "loss:  2.315666675567627\n",
      "loss:  2.298708915710449\n",
      "loss:  2.287222385406494\n",
      "loss:  2.331096649169922\n",
      "loss:  2.308847427368164\n",
      "loss:  2.308002471923828\n",
      "loss:  2.3114206790924072\n",
      "loss:  2.2665674686431885\n",
      "loss:  2.323694944381714\n",
      "loss:  2.3097236156463623\n",
      "loss:  2.307626485824585\n",
      "loss:  2.304978132247925\n",
      "loss:  2.288330316543579\n",
      "loss:  2.3070297241210938\n",
      "loss:  2.3126800060272217\n",
      "loss:  2.286630868911743\n",
      "loss:  2.3061699867248535\n",
      "loss:  2.313941478729248\n",
      "loss:  2.3247482776641846\n",
      "loss:  2.2719309329986572\n",
      "loss:  2.319566488265991\n",
      "loss:  2.2986855506896973\n",
      "loss:  2.323076009750366\n",
      "loss:  2.309267282485962\n",
      "loss:  2.30604887008667\n",
      "loss:  2.3040969371795654\n",
      "loss:  2.316977024078369\n",
      "loss:  2.31486439704895\n",
      "loss:  2.300818920135498\n",
      "loss:  2.314345598220825\n",
      "loss:  2.288022756576538\n",
      "loss:  2.299013137817383\n",
      "loss:  2.307887077331543\n",
      "loss:  2.294708728790283\n",
      "loss:  2.323308229446411\n",
      "loss:  2.297037363052368\n",
      "loss:  2.2816410064697266\n",
      "loss:  2.31941294670105\n",
      "loss:  2.288984537124634\n",
      "loss:  2.3222217559814453\n",
      "loss:  2.306997060775757\n",
      "loss:  2.286980628967285\n",
      "loss:  2.305143117904663\n",
      "loss:  2.292973041534424\n",
      "loss:  2.301677703857422\n",
      "loss:  2.321582317352295\n",
      "loss:  2.364687204360962\n",
      "loss:  2.346252918243408\n",
      "loss:  2.307507276535034\n",
      "loss:  2.3118302822113037\n",
      "loss:  2.3253121376037598\n",
      "loss:  2.326792001724243\n",
      "loss:  2.3240833282470703\n",
      "loss:  2.302884578704834\n",
      "loss:  2.298506498336792\n",
      "loss:  2.3150956630706787\n",
      "loss:  2.2963199615478516\n",
      "loss:  2.3010988235473633\n",
      "loss:  2.334476947784424\n",
      "loss:  2.3135197162628174\n",
      "loss:  2.3229188919067383\n",
      "loss:  2.3327414989471436\n",
      "loss:  2.2957754135131836\n",
      "loss:  2.281500816345215\n",
      "loss:  2.3186633586883545\n",
      "loss:  2.3068580627441406\n",
      "loss:  2.3135476112365723\n",
      "loss:  2.3083019256591797\n",
      "loss:  2.312593460083008\n",
      "loss:  2.3248565196990967\n",
      "loss:  2.304168462753296\n",
      "loss:  2.3064146041870117\n",
      "loss:  2.3345601558685303\n",
      "loss:  2.301924228668213\n",
      "loss:  2.2832961082458496\n",
      "loss:  2.281294345855713\n",
      "loss:  2.3002848625183105\n",
      "loss:  2.3026609420776367\n",
      "loss:  2.323265314102173\n",
      "loss:  2.293285608291626\n",
      "loss:  2.3187708854675293\n",
      "loss:  2.30182147026062\n",
      "loss:  2.3315978050231934\n",
      "loss:  2.2852752208709717\n",
      "loss:  2.2837235927581787\n",
      "loss:  2.3111515045166016\n",
      "loss:  2.301623582839966\n",
      "loss:  2.3104395866394043\n",
      "loss:  2.3189077377319336\n",
      "loss:  2.3031351566314697\n",
      "loss:  2.2982335090637207\n",
      "loss:  2.302792549133301\n",
      "loss:  2.288705825805664\n",
      "loss:  2.3276607990264893\n",
      "loss:  2.3161401748657227\n",
      "loss:  2.287172794342041\n",
      "loss:  2.3198513984680176\n",
      "loss:  2.3156490325927734\n",
      "loss:  2.298938751220703\n",
      "loss:  2.289855480194092\n",
      "loss:  2.2768521308898926\n",
      "loss:  2.2939369678497314\n",
      "loss:  2.313427448272705\n",
      "loss:  2.3036580085754395\n",
      "loss:  2.3037827014923096\n",
      "loss:  2.3024940490722656\n",
      "loss:  2.309739828109741\n",
      "loss:  2.30350661277771\n",
      "loss:  2.3376541137695312\n",
      "loss:  2.344743490219116\n",
      "loss:  2.334420680999756\n",
      "loss:  2.301765203475952\n",
      "loss:  2.3158371448516846\n",
      "loss:  2.314021110534668\n",
      "loss:  2.2791941165924072\n",
      "loss:  2.3109192848205566\n",
      "loss:  2.328814744949341\n",
      "loss:  2.2850663661956787\n",
      "loss:  2.2869064807891846\n",
      "loss:  2.2973549365997314\n",
      "loss:  2.309648036956787\n",
      "loss:  2.319917678833008\n",
      "loss:  2.2968192100524902\n",
      "loss:  2.3034632205963135\n",
      "loss:  2.3065638542175293\n",
      "loss:  2.295807361602783\n",
      "loss:  2.3041694164276123\n",
      "loss:  2.3253133296966553\n",
      "loss:  2.3065125942230225\n",
      "loss:  2.3173646926879883\n",
      "loss:  2.304535150527954\n",
      "loss:  2.3301949501037598\n",
      "loss:  2.3322556018829346\n",
      "loss:  2.3004326820373535\n",
      "loss:  2.3166017532348633\n",
      "loss:  2.3150508403778076\n",
      "loss:  2.3121018409729004\n",
      "loss:  2.303002119064331\n",
      "loss:  2.309326171875\n",
      "loss:  2.309166193008423\n",
      "loss:  2.293897867202759\n",
      "loss:  2.3221516609191895\n",
      "loss:  2.2998733520507812\n",
      "loss:  2.3295257091522217\n",
      "loss:  2.2999300956726074\n",
      "loss:  2.334766387939453\n",
      "loss:  2.3094000816345215\n",
      "loss:  2.328214168548584\n",
      "loss:  2.290505886077881\n",
      "loss:  2.324843168258667\n",
      "loss:  2.311370611190796\n",
      "loss:  2.340330123901367\n",
      "loss:  2.328327178955078\n",
      "loss:  2.2932560443878174\n",
      "loss:  2.3022801876068115\n",
      "loss:  2.3126752376556396\n",
      "loss:  2.3315696716308594\n",
      "loss:  2.2989273071289062\n",
      "loss:  2.3314571380615234\n",
      "loss:  2.3022704124450684\n",
      "loss:  2.3217124938964844\n",
      "loss:  2.306548833847046\n",
      "loss:  2.332793951034546\n",
      "loss:  2.2885892391204834\n",
      "loss:  2.297260284423828\n",
      "loss:  2.302196502685547\n",
      "loss:  2.318732738494873\n",
      "loss:  2.3316762447357178\n",
      "loss:  2.3190746307373047\n",
      "loss:  2.2900772094726562\n",
      "loss:  2.3346002101898193\n",
      "loss:  2.3295493125915527\n",
      "loss:  2.300105094909668\n",
      "loss:  2.3149566650390625\n",
      "loss:  2.2978811264038086\n",
      "loss:  2.308175802230835\n",
      "loss:  2.290809154510498\n",
      "loss:  2.310537338256836\n",
      "loss:  2.302994728088379\n",
      "loss:  2.3089675903320312\n",
      "loss:  2.307455539703369\n",
      "loss:  2.3000621795654297\n",
      "loss:  2.310979127883911\n",
      "loss:  2.307636260986328\n",
      "loss:  2.2989325523376465\n",
      "loss:  2.3099846839904785\n",
      "loss:  2.3105263710021973\n",
      "loss:  2.3176045417785645\n",
      "loss:  2.3057470321655273\n",
      "loss:  2.324615716934204\n",
      "loss:  2.317291021347046\n",
      "loss:  2.3014650344848633\n",
      "loss:  2.3182315826416016\n",
      "loss:  2.2880547046661377\n",
      "loss:  2.2924110889434814\n",
      "loss:  2.3221445083618164\n",
      "loss:  2.303731679916382\n",
      "loss:  2.284606456756592\n",
      "loss:  2.300664186477661\n",
      "loss:  2.3095147609710693\n",
      "loss:  2.313825845718384\n",
      "loss:  2.310948371887207\n",
      "loss:  2.315091609954834\n",
      "loss:  2.3310294151306152\n",
      "loss:  2.3510022163391113\n",
      "loss:  2.2865328788757324\n",
      "loss:  2.30560564994812\n",
      "loss:  2.28960919380188\n",
      "loss:  2.303579330444336\n",
      "loss:  2.2990169525146484\n",
      "loss:  2.291656970977783\n",
      "loss:  2.319885730743408\n",
      "loss:  2.3087258338928223\n",
      "loss:  2.317152976989746\n",
      "loss:  2.320605754852295\n",
      "loss:  2.315699338912964\n",
      "loss:  2.314749240875244\n",
      "loss:  2.30523419380188\n",
      "loss:  2.3228347301483154\n",
      "loss:  2.309187889099121\n",
      "loss:  2.3188443183898926\n",
      "loss:  2.2796764373779297\n",
      "loss:  2.277945041656494\n",
      "loss:  2.3160183429718018\n",
      "loss:  2.2954883575439453\n",
      "loss:  2.322988748550415\n",
      "loss:  2.3163232803344727\n",
      "loss:  2.322613477706909\n",
      "loss:  2.3010125160217285\n",
      "loss:  2.3118865489959717\n",
      "loss:  2.290818452835083\n",
      "loss:  2.317741870880127\n",
      "loss:  2.3328285217285156\n",
      "loss:  2.312420129776001\n",
      "loss:  2.312750816345215\n",
      "loss:  2.3304810523986816\n",
      "loss:  2.26965069770813\n",
      "loss:  2.318805456161499\n",
      "loss:  2.2987842559814453\n",
      "loss:  2.2966628074645996\n",
      "loss:  2.2994353771209717\n",
      "loss:  2.280773878097534\n",
      "loss:  2.328540325164795\n",
      "loss:  2.3365256786346436\n",
      "loss:  2.288952112197876\n",
      "loss:  2.322056293487549\n",
      "loss:  2.3275082111358643\n",
      "loss:  2.2976155281066895\n",
      "loss:  2.2870676517486572\n",
      "loss:  2.2948834896087646\n",
      "loss:  2.2928924560546875\n",
      "loss:  2.3369028568267822\n",
      "loss:  2.3149068355560303\n",
      "loss:  2.3069381713867188\n",
      "loss:  2.3234612941741943\n",
      "loss:  2.310325860977173\n",
      "loss:  2.2914328575134277\n",
      "loss:  2.319744110107422\n",
      "loss:  2.3026537895202637\n",
      "loss:  2.30946683883667\n",
      "loss:  2.2911229133605957\n",
      "loss:  2.312995195388794\n",
      "loss:  2.288546323776245\n",
      "loss:  2.3061466217041016\n",
      "loss:  2.288266897201538\n",
      "loss:  2.305079221725464\n",
      "loss:  2.3133392333984375\n",
      "loss:  2.310359477996826\n",
      "loss:  2.313851833343506\n",
      "loss:  2.3162872791290283\n",
      "loss:  2.316283702850342\n",
      "loss:  2.31787109375\n",
      "loss:  2.310112237930298\n",
      "loss:  2.3149662017822266\n",
      "loss:  2.3107290267944336\n",
      "loss:  2.323397397994995\n",
      "loss:  2.3369650840759277\n",
      "loss:  2.299914836883545\n",
      "loss:  2.325899839401245\n",
      "loss:  2.305506944656372\n",
      "loss:  2.2868728637695312\n",
      "loss:  2.3135547637939453\n",
      "loss:  2.3187673091888428\n",
      "loss:  2.338996648788452\n",
      "loss:  2.300396203994751\n",
      "loss:  2.3021793365478516\n",
      "loss:  2.296860933303833\n",
      "loss:  2.277752161026001\n",
      "loss:  2.3156049251556396\n",
      "loss:  2.309985876083374\n",
      "loss:  2.2959463596343994\n",
      "loss:  2.30692982673645\n",
      "loss:  2.3014235496520996\n",
      "loss:  2.2965424060821533\n",
      "loss:  2.3235819339752197\n",
      "loss:  2.286384344100952\n",
      "loss:  2.3149147033691406\n",
      "loss:  2.284942388534546\n",
      "loss:  2.3483126163482666\n",
      "loss:  2.3141698837280273\n",
      "loss:  2.31337308883667\n",
      "loss:  2.292311191558838\n",
      "loss:  2.313722848892212\n",
      "loss:  2.295377492904663\n",
      "loss:  2.3336832523345947\n",
      "loss:  2.284196138381958\n",
      "loss:  2.3404479026794434\n",
      "loss:  2.30668306350708\n",
      "loss:  2.307767391204834\n",
      "loss:  2.295673131942749\n",
      "loss:  2.3253955841064453\n",
      "loss:  2.310995578765869\n",
      "loss:  2.28324556350708\n",
      "loss:  2.3363404273986816\n",
      "loss:  2.3127851486206055\n",
      "loss:  2.297196865081787\n",
      "loss:  2.3333370685577393\n",
      "loss:  2.3022427558898926\n",
      "loss:  2.3231306076049805\n",
      "loss:  2.319991111755371\n",
      "loss:  2.318668842315674\n",
      "loss:  2.2962775230407715\n",
      "loss:  2.295987844467163\n",
      "loss:  2.327435255050659\n",
      "loss:  2.2932608127593994\n",
      "loss:  2.2816054821014404\n",
      "loss:  2.3272790908813477\n",
      "loss:  2.317467212677002\n",
      "loss:  2.3298022747039795\n",
      "loss:  2.32342791557312\n",
      "loss:  2.3023056983947754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3241405487060547\n",
      "loss:  2.3229446411132812\n",
      "loss:  2.3197286128997803\n",
      "loss:  2.323676824569702\n",
      "loss:  2.318359375\n",
      "loss:  2.293168306350708\n",
      "loss:  2.313380718231201\n",
      "loss:  2.2929041385650635\n",
      "loss:  2.3488247394561768\n",
      "loss:  2.2806577682495117\n",
      "loss:  2.302098035812378\n",
      "loss:  2.3019330501556396\n",
      "loss:  2.292318820953369\n",
      "loss:  2.3156967163085938\n",
      "loss:  2.2894976139068604\n",
      "loss:  2.306795120239258\n",
      "loss:  2.3125803470611572\n",
      "loss:  2.294137716293335\n",
      "loss:  2.2964560985565186\n",
      "loss:  2.3110811710357666\n",
      "loss:  2.312286615371704\n",
      "loss:  2.2983601093292236\n",
      "loss:  2.3121933937072754\n",
      "loss:  2.314962387084961\n",
      "loss:  2.3234524726867676\n",
      "loss:  2.3110642433166504\n",
      "loss:  2.310222864151001\n",
      "loss:  2.2854862213134766\n",
      "loss:  2.3091044425964355\n",
      "loss:  2.298856258392334\n",
      "loss:  2.307724714279175\n",
      "loss:  2.311861991882324\n",
      "loss:  2.31479549407959\n",
      "loss:  2.301307201385498\n",
      "loss:  2.322693347930908\n",
      "loss:  2.2877461910247803\n",
      "loss:  2.305274248123169\n",
      "loss:  2.273602247238159\n",
      "loss:  2.320629835128784\n",
      "loss:  2.3094775676727295\n",
      "loss:  2.294339895248413\n",
      "loss:  2.304098129272461\n",
      "loss:  2.318307638168335\n",
      "loss:  2.31197190284729\n",
      "loss:  2.3144314289093018\n",
      "loss:  2.285322904586792\n",
      "loss:  2.314908266067505\n",
      "loss:  2.3241608142852783\n",
      "loss:  2.325420618057251\n",
      "loss:  2.3145439624786377\n",
      "loss:  2.317225694656372\n",
      "loss:  2.277920961380005\n",
      "loss:  2.2861475944519043\n",
      "loss:  2.298898935317993\n",
      "loss:  2.335015296936035\n",
      "loss:  2.316664457321167\n",
      "loss:  2.299901008605957\n",
      "loss:  2.287182569503784\n",
      "loss:  2.298696994781494\n",
      "loss:  2.317225456237793\n",
      "loss:  2.2885189056396484\n",
      "loss:  2.325896978378296\n",
      "loss:  2.3165407180786133\n",
      "loss:  2.287335157394409\n",
      "loss:  2.3134920597076416\n",
      "loss:  2.2975149154663086\n",
      "loss:  2.2653071880340576\n",
      "loss:  2.3012735843658447\n",
      "loss:  2.2957563400268555\n",
      "loss:  2.2906076908111572\n",
      "loss:  2.3274447917938232\n",
      "loss:  2.3191473484039307\n",
      "loss:  2.316737651824951\n",
      "loss:  2.320072889328003\n",
      "loss:  2.323784589767456\n",
      "loss:  2.3001649379730225\n",
      "loss:  2.2883541584014893\n",
      "loss:  2.30299973487854\n",
      "loss:  2.29880952835083\n",
      "loss:  2.2823030948638916\n",
      "loss:  2.2653212547302246\n",
      "loss:  2.2708628177642822\n",
      "loss:  2.3138587474823\n",
      "loss:  2.3050854206085205\n",
      "loss:  2.324262857437134\n",
      "loss:  2.284881830215454\n",
      "loss:  2.295574426651001\n",
      "loss:  2.2957494258880615\n",
      "loss:  2.3018593788146973\n",
      "loss:  2.3124544620513916\n",
      "loss:  2.2995030879974365\n",
      "loss:  2.291618824005127\n",
      "loss:  2.3034703731536865\n",
      "loss:  2.2971155643463135\n",
      "loss:  2.303969621658325\n",
      "loss:  2.327667236328125\n",
      "loss:  2.288466215133667\n",
      "loss:  2.2768092155456543\n",
      "loss:  2.3052492141723633\n",
      "loss:  2.2907450199127197\n",
      "loss:  2.318761110305786\n",
      "loss:  2.331310272216797\n",
      "loss:  2.3006560802459717\n",
      "loss:  2.2956464290618896\n",
      "loss:  2.304333209991455\n",
      "loss:  2.321105718612671\n",
      "loss:  2.2840864658355713\n",
      "loss:  2.3248279094696045\n",
      "loss:  2.302894353866577\n",
      "loss:  2.299654960632324\n",
      "loss:  2.3014025688171387\n",
      "loss:  2.3167128562927246\n",
      "loss:  2.2752578258514404\n",
      "loss:  2.2925541400909424\n",
      "loss:  2.3077476024627686\n",
      "loss:  2.3247385025024414\n",
      "loss:  2.2901012897491455\n",
      "loss:  2.297196626663208\n",
      "loss:  2.2785181999206543\n",
      "loss:  2.3066980838775635\n",
      "loss:  2.3111255168914795\n",
      "loss:  2.31713604927063\n",
      "loss:  2.3184654712677\n",
      "loss:  2.301736354827881\n",
      "loss:  2.302530527114868\n",
      "loss:  2.333972454071045\n",
      "loss:  2.3137094974517822\n",
      "loss:  2.3014838695526123\n",
      "loss:  2.312739610671997\n",
      "loss:  2.3158340454101562\n",
      "loss:  2.323157548904419\n",
      "loss:  2.284104585647583\n",
      "loss:  2.3058764934539795\n",
      "loss:  2.3115930557250977\n",
      "loss:  2.281395673751831\n",
      "loss:  2.3211750984191895\n",
      "loss:  2.3460192680358887\n",
      "loss:  2.3203272819519043\n",
      "loss:  2.2783737182617188\n",
      "loss:  2.3143649101257324\n",
      "loss:  2.2995152473449707\n",
      "loss:  2.31821346282959\n",
      "loss:  2.2855334281921387\n",
      "loss:  2.309108257293701\n",
      "loss:  2.29716420173645\n",
      "loss:  2.303192615509033\n",
      "loss:  2.3048183917999268\n",
      "loss:  2.287259101867676\n",
      "loss:  2.305330991744995\n",
      "loss:  2.325819730758667\n",
      "loss:  2.3087432384490967\n",
      "loss:  2.3034894466400146\n",
      "loss:  2.2869982719421387\n",
      "loss:  2.324638843536377\n",
      "loss:  2.3152146339416504\n",
      "loss:  2.2973663806915283\n",
      "loss:  2.3140718936920166\n",
      "loss:  2.2798471450805664\n",
      "loss:  2.295408010482788\n",
      "loss:  2.3208603858947754\n",
      "loss:  2.2974085807800293\n",
      "loss:  2.284311532974243\n",
      "loss:  2.3009660243988037\n",
      "loss:  2.322270631790161\n",
      "loss:  2.311955451965332\n",
      "loss:  2.3063247203826904\n",
      "loss:  2.288646697998047\n",
      "loss:  2.303281307220459\n",
      "loss:  2.297679901123047\n",
      "loss:  2.3019678592681885\n",
      "loss:  2.3146910667419434\n",
      "loss:  2.2914445400238037\n",
      "loss:  2.2992002964019775\n",
      "loss:  2.3146843910217285\n",
      "loss:  2.28830885887146\n",
      "loss:  2.2909328937530518\n",
      "loss:  2.2961394786834717\n",
      "loss:  2.3181192874908447\n",
      "loss:  2.308321475982666\n",
      "loss:  2.2840676307678223\n",
      "loss:  2.302980661392212\n",
      "loss:  2.302375316619873\n",
      "loss:  2.3070266246795654\n",
      "loss:  2.3227767944335938\n",
      "loss:  2.3060803413391113\n",
      "loss:  2.2996628284454346\n",
      "loss:  2.2965540885925293\n",
      "loss:  2.307310104370117\n",
      "loss:  2.3074936866760254\n",
      "loss:  2.2781081199645996\n",
      "loss:  2.3142738342285156\n",
      "loss:  2.3122472763061523\n",
      "loss:  2.306619882583618\n",
      "loss:  2.3171355724334717\n",
      "loss:  2.3252692222595215\n",
      "loss:  2.294039249420166\n",
      "loss:  2.2888362407684326\n",
      "loss:  2.2935757637023926\n",
      "loss:  2.290114641189575\n",
      "loss:  2.287595272064209\n",
      "loss:  2.298590898513794\n",
      "loss:  2.312893867492676\n",
      "loss:  2.2990219593048096\n",
      "loss:  2.2961618900299072\n",
      "loss:  2.2861533164978027\n",
      "loss:  2.2874982357025146\n",
      "loss:  2.3062705993652344\n",
      "loss:  2.299037456512451\n",
      "loss:  2.295024871826172\n",
      "loss:  2.3168747425079346\n",
      "loss:  2.3013429641723633\n",
      "loss:  2.2911252975463867\n",
      "loss:  2.2978498935699463\n",
      "loss:  2.3152058124542236\n",
      "loss:  2.314133644104004\n",
      "loss:  2.301258087158203\n",
      "loss:  2.280486822128296\n",
      "loss:  2.291127920150757\n",
      "loss:  2.2976796627044678\n",
      "loss:  2.296415090560913\n",
      "loss:  2.3018205165863037\n",
      "loss:  2.294739007949829\n",
      "loss:  2.2835679054260254\n",
      "loss:  2.279548406600952\n",
      "loss:  2.301661729812622\n",
      "loss:  2.2829465866088867\n",
      "loss:  2.3046159744262695\n",
      "loss:  2.310084104537964\n",
      "loss:  2.2989110946655273\n",
      "loss:  2.286376476287842\n",
      "loss:  2.28143572807312\n",
      "loss:  2.3099257946014404\n",
      "loss:  2.291687488555908\n",
      "loss:  2.307025194168091\n",
      "loss:  2.305058002471924\n",
      "loss:  2.2900893688201904\n",
      "loss:  2.2903852462768555\n",
      "loss:  2.3038179874420166\n",
      "loss:  2.300704002380371\n",
      "loss:  2.2956721782684326\n",
      "loss:  2.28580904006958\n",
      "loss:  2.278454303741455\n",
      "loss:  2.294084310531616\n",
      "loss:  2.314779758453369\n",
      "loss:  2.284480094909668\n",
      "loss:  2.288134813308716\n",
      "loss:  2.2851550579071045\n",
      "loss:  2.29125714302063\n",
      "loss:  2.2859950065612793\n",
      "loss:  2.291027069091797\n",
      "loss:  2.291109085083008\n",
      "loss:  2.2771811485290527\n",
      "loss:  2.273347854614258\n",
      "loss:  2.3111112117767334\n",
      "loss:  2.3013224601745605\n",
      "loss:  2.298058271408081\n",
      "loss:  2.2686352729797363\n",
      "loss:  2.3044321537017822\n",
      "loss:  2.2809817790985107\n",
      "loss:  2.2711985111236572\n",
      "loss:  2.284871816635132\n",
      "loss:  2.275221824645996\n",
      "loss:  2.3014161586761475\n",
      "loss:  2.298473358154297\n",
      "loss:  2.2903852462768555\n",
      "loss:  2.2849838733673096\n",
      "loss:  2.2868614196777344\n",
      "loss:  2.280482530593872\n",
      "loss:  2.2863781452178955\n",
      "loss:  2.2869129180908203\n",
      "loss:  2.264918327331543\n",
      "loss:  2.3164849281311035\n",
      "loss:  2.306140899658203\n",
      "loss:  2.2807445526123047\n",
      "loss:  2.303947925567627\n",
      "loss:  2.288729190826416\n",
      "loss:  2.2949001789093018\n",
      "loss:  2.2721219062805176\n",
      "loss:  2.279679775238037\n",
      "loss:  2.280282735824585\n",
      "loss:  2.2498979568481445\n",
      "loss:  2.2878711223602295\n",
      "loss:  2.262397527694702\n",
      "loss:  2.2877001762390137\n",
      "loss:  2.263662338256836\n",
      "loss:  2.283160924911499\n",
      "loss:  2.2768020629882812\n",
      "loss:  2.274221658706665\n",
      "loss:  2.291442394256592\n",
      "loss:  2.288928508758545\n",
      "loss:  2.295210599899292\n",
      "loss:  2.274573802947998\n",
      "loss:  2.2795674800872803\n",
      "loss:  2.2821204662323\n",
      "loss:  2.27193021774292\n",
      "loss:  2.273878812789917\n",
      "loss:  2.2892868518829346\n",
      "loss:  2.2702479362487793\n",
      "loss:  2.256943941116333\n",
      "loss:  2.2914414405822754\n",
      "loss:  2.2977707386016846\n",
      "loss:  2.272580623626709\n",
      "loss:  2.261735439300537\n",
      "loss:  2.2775650024414062\n",
      "loss:  2.2692131996154785\n",
      "loss:  2.2676315307617188\n",
      "loss:  2.2546958923339844\n",
      "loss:  2.2759158611297607\n",
      "loss:  2.2618212699890137\n",
      "loss:  2.263939380645752\n",
      "loss:  2.2564547061920166\n",
      "loss:  2.254232883453369\n",
      "loss:  2.2701566219329834\n",
      "loss:  2.2506210803985596\n",
      "loss:  2.24737548828125\n",
      "loss:  2.2733747959136963\n",
      "loss:  2.2597293853759766\n",
      "loss:  2.257642984390259\n",
      "loss:  2.241753339767456\n",
      "loss:  2.228729486465454\n",
      "loss:  2.2547972202301025\n",
      "loss:  2.268001079559326\n",
      "loss:  2.245725393295288\n",
      "loss:  2.238070249557495\n",
      "loss:  2.224985361099243\n",
      "loss:  2.2166128158569336\n",
      "loss:  2.248014450073242\n",
      "loss:  2.232922315597534\n",
      "loss:  2.223125457763672\n",
      "loss:  2.230245351791382\n",
      "loss:  2.2247772216796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.237356662750244\n",
      "loss:  2.2285852432250977\n",
      "loss:  2.1872293949127197\n",
      "loss:  2.1636691093444824\n",
      "loss:  2.1678011417388916\n",
      "loss:  2.217186450958252\n",
      "loss:  2.148378610610962\n",
      "loss:  2.187135696411133\n",
      "loss:  2.1931533813476562\n",
      "loss:  2.169384479522705\n",
      "loss:  2.192286968231201\n",
      "loss:  2.1686031818389893\n",
      "loss:  2.134326219558716\n",
      "loss:  2.1273372173309326\n",
      "loss:  2.1304595470428467\n",
      "loss:  2.0746350288391113\n",
      "loss:  2.1058189868927\n",
      "loss:  2.120042562484741\n",
      "loss:  1.9664359092712402\n",
      "loss:  2.095496654510498\n",
      "loss:  2.081892967224121\n",
      "loss:  2.0101840496063232\n",
      "loss:  1.9077972173690796\n",
      "loss:  1.8225200176239014\n",
      "loss:  1.8706345558166504\n",
      "loss:  1.9103792905807495\n",
      "loss:  1.8343061208724976\n",
      "loss:  1.7972294092178345\n",
      "loss:  1.768644094467163\n",
      "loss:  1.9256001710891724\n",
      "loss:  1.7056732177734375\n",
      "loss:  1.7497074604034424\n",
      "loss:  1.684014916419983\n",
      "loss:  1.4352012872695923\n",
      "loss:  1.6212260723114014\n",
      "loss:  1.5334924459457397\n",
      "loss:  1.5171996355056763\n",
      "loss:  1.6296522617340088\n",
      "loss:  1.543622612953186\n",
      "loss:  1.6827988624572754\n",
      "loss:  1.6637877225875854\n",
      "loss:  1.6409461498260498\n",
      "loss:  1.2840955257415771\n",
      "loss:  1.4131131172180176\n",
      "loss:  1.4384976625442505\n",
      "loss:  1.521999478340149\n",
      "loss:  1.3418940305709839\n",
      "loss:  1.7890324592590332\n",
      "loss:  1.699772834777832\n",
      "loss:  1.6480724811553955\n",
      "loss:  1.307289958000183\n",
      "loss:  1.250245451927185\n",
      "loss:  1.1709330081939697\n",
      "loss:  1.5496677160263062\n",
      "loss:  1.4901273250579834\n",
      "loss:  1.7100410461425781\n",
      "loss:  1.4365909099578857\n",
      "loss:  1.3725721836090088\n",
      "loss:  1.3309555053710938\n",
      "loss:  1.236348032951355\n",
      "loss:  1.4461003541946411\n",
      "loss:  1.3537639379501343\n",
      "loss:  1.3112024068832397\n",
      "loss:  1.69297194480896\n",
      "loss:  1.150614857673645\n",
      "loss:  1.275160312652588\n",
      "loss:  1.8395406007766724\n",
      "loss:  1.2400133609771729\n",
      "loss:  1.4223862886428833\n",
      "loss:  1.408990502357483\n",
      "loss:  1.373302698135376\n",
      "loss:  1.2673742771148682\n",
      "loss:  1.7176545858383179\n",
      "loss:  2.3140311241149902\n",
      "loss:  1.572867751121521\n",
      "loss:  1.22394597530365\n",
      "loss:  1.3977261781692505\n",
      "loss:  1.3717055320739746\n",
      "loss:  1.170602798461914\n",
      "loss:  1.152152180671692\n",
      "loss:  1.289159893989563\n",
      "loss:  1.341078281402588\n",
      "loss:  1.2643933296203613\n",
      "loss:  1.4749014377593994\n",
      "loss:  1.3088114261627197\n",
      "loss:  1.3211777210235596\n",
      "loss:  1.3201727867126465\n",
      "loss:  1.0857477188110352\n",
      "loss:  0.9015979170799255\n",
      "loss:  1.0687897205352783\n",
      "loss:  1.2140839099884033\n",
      "loss:  1.0680845975875854\n",
      "loss:  1.4490606784820557\n",
      "loss:  1.407658338546753\n",
      "loss:  1.199257731437683\n",
      "loss:  1.7542724609375\n",
      "loss:  1.1545419692993164\n",
      "loss:  1.3420181274414062\n",
      "loss:  1.3387227058410645\n",
      "loss:  0.932675302028656\n",
      "loss:  1.4404962062835693\n",
      "loss:  1.2442781925201416\n",
      "loss:  1.1111834049224854\n",
      "loss:  1.4590189456939697\n",
      "loss:  1.332938551902771\n",
      "loss:  1.0415161848068237\n",
      "loss:  1.1824219226837158\n",
      "loss:  1.1484665870666504\n",
      "loss:  1.2241487503051758\n",
      "loss:  1.599190354347229\n",
      "loss:  1.431110143661499\n",
      "loss:  1.1656957864761353\n",
      "loss:  1.2515538930892944\n",
      "loss:  1.2332491874694824\n",
      "loss:  1.3947665691375732\n",
      "loss:  1.1997096538543701\n",
      "loss:  1.913278579711914\n",
      "loss:  1.7409712076187134\n",
      "loss:  1.5592995882034302\n",
      "loss:  1.4130377769470215\n",
      "loss:  1.2873245477676392\n",
      "loss:  1.0938589572906494\n",
      "loss:  0.94407719373703\n",
      "loss:  1.190812110900879\n",
      "loss:  1.4105010032653809\n",
      "loss:  1.5589739084243774\n",
      "loss:  1.3014116287231445\n",
      "loss:  1.294091820716858\n",
      "loss:  1.2471933364868164\n",
      "loss:  1.0822535753250122\n",
      "loss:  1.2882883548736572\n",
      "loss:  1.1868019104003906\n",
      "loss:  0.9866101741790771\n",
      "loss:  1.5356285572052002\n",
      "loss:  1.3549031019210815\n",
      "loss:  1.0459266901016235\n",
      "loss:  0.8838245272636414\n",
      "loss:  1.1454977989196777\n",
      "loss:  1.1310014724731445\n",
      "loss:  1.2129182815551758\n",
      "loss:  0.8894719481468201\n",
      "loss:  1.3206530809402466\n",
      "loss:  1.5435149669647217\n",
      "loss:  1.2363308668136597\n",
      "loss:  1.1058090925216675\n",
      "loss:  0.9872994422912598\n",
      "loss:  1.1227021217346191\n",
      "loss:  0.7646189332008362\n",
      "loss:  1.0541374683380127\n",
      "loss:  0.9498367309570312\n",
      "loss:  0.8642011284828186\n",
      "loss:  1.61203134059906\n",
      "loss:  1.6919649839401245\n",
      "loss:  1.4532369375228882\n",
      "loss:  0.9967570304870605\n",
      "loss:  1.0349587202072144\n",
      "loss:  0.8147633671760559\n",
      "loss:  1.139404296875\n",
      "loss:  1.7428498268127441\n",
      "loss:  1.4988040924072266\n",
      "loss:  1.5559276342391968\n",
      "loss:  1.1969916820526123\n",
      "loss:  1.25210440158844\n",
      "loss:  2.020885467529297\n",
      "loss:  1.688734531402588\n",
      "loss:  1.0400851964950562\n",
      "loss:  1.326319932937622\n",
      "loss:  1.090126395225525\n",
      "loss:  1.0884754657745361\n",
      "loss:  1.0811325311660767\n",
      "loss:  1.0833770036697388\n",
      "loss:  1.068167805671692\n",
      "loss:  0.8113198280334473\n",
      "loss:  1.1707584857940674\n",
      "loss:  1.5361219644546509\n",
      "loss:  1.0956617593765259\n",
      "loss:  0.9763430953025818\n",
      "loss:  1.0584338903427124\n",
      "loss:  1.1883772611618042\n",
      "loss:  0.9572672247886658\n",
      "loss:  0.8447027206420898\n",
      "loss:  1.2594350576400757\n",
      "loss:  1.4311145544052124\n",
      "loss:  2.187181234359741\n",
      "loss:  1.3411351442337036\n",
      "loss:  1.6523280143737793\n",
      "loss:  1.1456298828125\n",
      "loss:  0.985053300857544\n",
      "loss:  0.9341833591461182\n",
      "loss:  1.0849435329437256\n",
      "loss:  1.2789429426193237\n",
      "loss:  0.9594171643257141\n",
      "loss:  1.001818060874939\n",
      "loss:  0.9719036817550659\n",
      "loss:  1.4496363401412964\n",
      "loss:  1.0071024894714355\n",
      "loss:  1.0819412469863892\n",
      "loss:  0.9808142185211182\n",
      "loss:  1.0334632396697998\n",
      "loss:  1.0258287191390991\n",
      "loss:  1.1054083108901978\n",
      "loss:  0.8463632464408875\n",
      "loss:  0.7209236025810242\n",
      "loss:  1.014989972114563\n",
      "loss:  1.2155184745788574\n",
      "loss:  1.03480863571167\n",
      "loss:  0.82020103931427\n",
      "loss:  0.683225154876709\n",
      "loss:  0.9345869421958923\n",
      "loss:  1.5104153156280518\n",
      "loss:  1.1735345125198364\n",
      "loss:  0.7854124903678894\n",
      "loss:  0.8175318837165833\n",
      "loss:  1.6076096296310425\n",
      "loss:  1.3032127618789673\n",
      "loss:  0.9514743089675903\n",
      "loss:  0.6317470073699951\n",
      "loss:  0.7666009068489075\n",
      "loss:  0.6518730521202087\n",
      "loss:  1.0018916130065918\n",
      "loss:  1.3350937366485596\n",
      "loss:  1.2977014780044556\n",
      "loss:  1.1711794137954712\n",
      "loss:  1.2137333154678345\n",
      "loss:  0.9755317568778992\n",
      "loss:  1.3484328985214233\n",
      "loss:  0.796337902545929\n",
      "loss:  1.0395840406417847\n",
      "loss:  0.6846697330474854\n",
      "loss:  0.772120475769043\n",
      "loss:  0.9928397536277771\n",
      "loss:  0.9693446159362793\n",
      "loss:  0.7917971611022949\n",
      "loss:  0.8683655858039856\n",
      "loss:  0.6448034048080444\n",
      "loss:  0.6967357993125916\n",
      "loss:  1.1125102043151855\n",
      "loss:  1.488690972328186\n",
      "loss:  1.3710211515426636\n",
      "loss:  2.4012205600738525\n",
      "loss:  1.4601085186004639\n",
      "loss:  1.311954379081726\n",
      "loss:  1.1092442274093628\n",
      "loss:  0.8123988509178162\n",
      "loss:  0.9130462408065796\n",
      "loss:  1.2450122833251953\n",
      "loss:  1.9508490562438965\n",
      "loss:  1.2886017560958862\n",
      "loss:  1.7346605062484741\n",
      "loss:  1.7795665264129639\n",
      "loss:  1.7552350759506226\n",
      "loss:  1.651120662689209\n",
      "loss:  1.2875275611877441\n",
      "loss:  1.3546336889266968\n",
      "loss:  1.6061762571334839\n",
      "loss:  1.1658251285552979\n",
      "loss:  0.9727553129196167\n",
      "loss:  0.7195366621017456\n",
      "loss:  1.0782912969589233\n",
      "loss:  1.6845372915267944\n",
      "loss:  1.3140745162963867\n",
      "loss:  0.9209532141685486\n",
      "loss:  0.9243818521499634\n",
      "loss:  1.1485884189605713\n",
      "loss:  1.2617881298065186\n",
      "loss:  0.7768816351890564\n",
      "loss:  0.6943151354789734\n",
      "loss:  1.1764613389968872\n",
      "loss:  1.6750736236572266\n",
      "loss:  2.0018162727355957\n",
      "loss:  1.436863899230957\n",
      "loss:  0.8605191707611084\n",
      "loss:  0.8189194202423096\n",
      "loss:  1.683863639831543\n",
      "loss:  1.5210070610046387\n",
      "loss:  0.9619945883750916\n",
      "loss:  0.9575901627540588\n",
      "loss:  1.517837643623352\n",
      "loss:  1.1215482950210571\n",
      "loss:  1.949375033378601\n",
      "loss:  2.818406581878662\n",
      "loss:  2.5129127502441406\n",
      "loss:  2.6951496601104736\n",
      "loss:  2.245628833770752\n",
      "loss:  2.228888511657715\n",
      "loss:  2.3621068000793457\n",
      "loss:  2.1926558017730713\n",
      "loss:  1.9572796821594238\n",
      "loss:  1.9274383783340454\n",
      "loss:  5.495205879211426\n",
      "loss:  2.1670496463775635\n",
      "loss:  1.8494387865066528\n",
      "loss:  1.4431219100952148\n",
      "loss:  1.7813193798065186\n",
      "loss:  1.5374873876571655\n",
      "loss:  1.7009776830673218\n",
      "loss:  1.718505859375\n",
      "loss:  1.234931230545044\n",
      "loss:  1.3179422616958618\n",
      "loss:  1.3744038343429565\n",
      "loss:  1.5962966680526733\n",
      "loss:  1.2301013469696045\n",
      "loss:  1.4997678995132446\n",
      "loss:  1.9190295934677124\n",
      "loss:  1.4474366903305054\n",
      "loss:  3.8576231002807617\n",
      "loss:  1.985158920288086\n",
      "loss:  2.165794610977173\n",
      "loss:  1.9251576662063599\n",
      "loss:  1.73013174533844\n",
      "loss:  1.683704137802124\n",
      "loss:  1.9541791677474976\n",
      "loss:  2.839859962463379\n",
      "loss:  13.349720001220703\n",
      "loss:  95.96675872802734\n",
      "loss:  195.52732849121094\n"
     ]
    }
   ],
   "source": [
    "model_lrfinder = FashionMnistNet()\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "log_lrs, losses = find_lr(model_lrfinder, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x128500e48>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([10**x for x in log_lrs], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
