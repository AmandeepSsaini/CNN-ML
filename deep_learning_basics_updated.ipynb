{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** code below uses [PyTorch](https://pytorch.org/tutorials/) but the ideas are similar in most of the other deep learning tools ([TensorFlow](https://www.tensorflow.org/tutorials), [MXNet](http://d2l.ai/) ... etc.)**\n",
    "\n",
    "Text content (like this cell) is written in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Highly recommended - FastAI course](https://course.fast.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import tensor\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train_data = pd.read_csv('fashion-mnist_train.csv')\n",
    "    test_data = pd.read_csv('fashion-mnist_test.csv')\n",
    "    x_train = train_data[train_data.columns[1:]].values\n",
    "    y_train = train_data.label.values\n",
    "    x_test = test_data[test_data.columns[1:]].values\n",
    "    y_test = test_data.label.values\n",
    "    return map(tensor, (x_train, y_train, x_test, y_test)) # maps are useful functions to know\n",
    "                                                           # here, we are just converting lists to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_data()\n",
    "train_n, train_m = x_train.shape\n",
    "test_n, test_m = x_test.shape\n",
    "n_cls = y_train.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpl.rcParams['image.cmap'] = 'gray' # it is good to try different ways to visualize your data \n",
    "                                    # matplotlib is a good library although its interface is pretty bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12273f7b8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[torch.randint(train_n, (1,))].view(28, 28)) # visualize a random image in the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](lenet.jpg \"lenet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the model\n",
    "class FashionMnistNet(nn.Module):\n",
    "    # Based on Lecunn's Lenet architecture\n",
    "    def __init__(self):\n",
    "        super(FashionMnistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 5, 2, 2) # convolution demo (http://cs231n.github.io/convolutional-networks/)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, 2, 1) \n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, 2, 1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, 2, 1)\n",
    "        self.fc1 = nn.Linear(32*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 40)\n",
    "        self.fc4 = nn.Linear(40, 10)\n",
    "#         self.adp_svg_pool = nn.AdaptiveAvgPool2d()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-97d288d17ada>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-97d288d17ada>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    instantiating the model\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# instantiating the model\n",
    "# model = FashionMnistNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.forward(x_train.float().reshape(train_n, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Numerical Stability **\n",
    "- Remember to keep all your values float wherever required (you might get wrong/undesirable behavior with integer values). Try running the next two cells for an example and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FashionMnistNet()\n",
    "# model.forward(x_train[0].reshape(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.forward(x_train[0].float().reshape(1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question - Why shouldn't we initialize neural network weights to zero?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One good initialization (used as default in pytorch) is kaiming initialization\n",
    "\n",
    "- [Kaiming initialization of weights](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "- [Blogpost explaining kaiming](https://pouannes.github.io/blog/initialization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization is so important that you might not even require any normalization layer like batchnorm\n",
    "\n",
    "[See Fixup initialization](https://openreview.net/forum?id=H1gsz30cKX) (10,000 layers) **(°◇°)\t**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionMnistNet() # Creating a model\n",
    "lr = 0.05 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 32 # batch size \n",
    "loss_func = F.cross_entropy # loss function \n",
    "opt = optim.Adam(model.parameters(), lr=lr) # optimizer\n",
    "accuracy_vals = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Why shouldn't we use large batch sizes?](https://arxiv.org/abs/1609.04836)\n",
    "\n",
    "[There is an answer on stackoverflow that explains the same](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [32 x 64], m2: [800 x 120] at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/TH/generic/THTensorMath.cpp:197",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7951c9af3301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# model.forward(xb) computes the prediction of model on given input xb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backpropagating the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-be76bcebfe2c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_flat_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [32 x 64], m2: [800 x 120] at /Users/distiller/project/conda/conda-bld/pytorch_1573049287641/work/aten/src/TH/generic/THTensorMath.cpp:197"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    #print(model.training)\n",
    "    for i in range((train_n-1)//bs + 1): # (train_n-1)//bs equals the number of batches when we divide the divide by given batch size bs \n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        # Pytorch reshape function has four arguments -  (batchsize, number of channels, width, height)\n",
    "        xb = x_train[start_i:end_i].float().reshape(bs, 1, 28, 28) \n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model.forward(xb), yb) # model.forward(xb) computes the prediction of model on given input xb\n",
    "        loss.backward() # backpropagating the gradients\n",
    "        opt.step() # gradient descent \n",
    "        opt.zero_grad() # don't forget to add this line after each batch (zero out the gradients)\n",
    "        \n",
    "    model.eval()\n",
    "    #print(model.training)\n",
    "    with torch.no_grad(): # this line essentially tells pytorch don't compute the gradients for test case\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].float().reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals.append((accuracy*100/test_n).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x124cd0eb8>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization is very important!\n",
    "[Why?](https://www.youtube.com/watch?v=FDCfw-YqWTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(72.9505), tensor(89.9669))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Normalization\n",
    "x_train, x_test = x_train.float(), x_test.float()\n",
    "train_mean,train_std = x_train.mean(),x_train.std()\n",
    "train_mean,train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, m, s): return (x-m)/s\n",
    "x_train = normalize(x_train, train_mean, train_std)\n",
    "x_test = normalize(x_test, train_mean, train_std) # note this normalize test data also with training mean and standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  83.47000122070312\n",
      "Accuracy:  87.26000213623047\n",
      "Accuracy:  88.06999969482422\n",
      "Accuracy:  88.5999984741211\n",
      "Accuracy:  89.02999877929688\n",
      "Accuracy:  88.98999786376953\n",
      "Accuracy:  89.37000274658203\n",
      "Accuracy:  89.47000122070312\n",
      "Accuracy:  89.66000366210938\n",
      "Accuracy:  89.5999984741211\n"
     ]
    }
   ],
   "source": [
    "model_wnd = FashionMnistNet()\n",
    "lr = 0.05 # learning rate\n",
    "epochs = 10 # number of epochs\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "opt = optim.SGD(model_wnd.parameters(), lr=lr)\n",
    "accuracy_vals_wnd = []\n",
    "for epoch in range(epochs):\n",
    "    model_wnd.train()\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        loss = loss_func(model_wnd.forward(xb), yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    model_wnd.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, accuracy = 0., 0.\n",
    "        validation_size = int(test_n/10)\n",
    "        for i in range(test_n):\n",
    "            x = x_test[i].reshape(1, 1, 28, 28)\n",
    "            y = y_test[i]\n",
    "            pred = model_wnd.forward(x)\n",
    "            accuracy += (torch.argmax(pred) == y).float()\n",
    "        print(\"Accuracy: \", (accuracy*100/test_n).item())\n",
    "        accuracy_vals_wnd.append((accuracy*100/test_n).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1227e1208>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAcSElEQVR4nO3deXRcd5nm8e8rlfbFm+RNtuI9jrM6KLFj4kDHYWkSOoHpnA5JZgamITMwdCCnB2iYbjzdB2aYIUMTmG7O8UAzf2ACwdkYIJBJ4EgJISLyQuLETsmxYtmybJck21qsteqdP6piZEeOyrFKt27V8zlHx67NenyP9dT1W797r7k7IiISPgVBBxARkbdHBS4iElIqcBGRkFKBi4iElApcRCSkItP5zWpqanzJkiXT+S1FREJv+/btXe5ee/b901rgS5YsoaWlZTq/pYhI6JnZgYnu1whFRCSkVOAiIiGlAhcRCSkVuIhISKnARURCSgUuIhJSKnARkZCa1nXgIiKZFk84B7oH2Hukj/2xfooKC6gsjVBVWkRVSYSq0sjp25UlESpLIhQWWNCx3xYVuIiE1vGBEfYe6WPvkV72diZ/ffVoH0OjifP6cyqKC5OFXpoq+JII1amCH1/4E70BVKdulxUVYja9bwQqcBHJeqPxBPtjA+w90suezj8W9pHeodPPmV1RzCULqrhr3UWsnl/FJQuqWTG3koQ7fUNjqa9R+oeTv+8fGqP3rNt9w6Onn9t5cij5/KExBkbik2YsLLA/Fv4bbwDj3hDu3bSSedWlU7pdVOAikjXcnVjfMHuO9LG3s5e9R/rY09nLa7F+RuPJq4cVFRor5laxYfkcVi+oYvX8alYvqKK2suSce8DlxRHmVb/9XPGEp4p+kjeAoVH6xt0+1jfE/ljy9r+/YfnbD3AOKnARCcTQaJzWo/3sGTf+2Hukj56BkdPPmV9dyuoFVbz74rlckirrZbUVFBVO7/qLwgJjRlkRM8qKpvX7TkYFLiIZ5e4cOj7I3iN9vHqk9/TedVvXAInUJXlLiwq4eH41710zj9Xzq1i9oJrV86uYWV4cbPgspwIXkQt2amSMWN/w6a+jvUPsi/Wzt7OPV4/00Tc8dvq59bPLWT2/ipuvWMglqbKun10e2pUgQVKBi8iExuIJegZGONY3TKx/mFhv6tdxRX2sb4hY3/CEH/JVlURYvaCK29bWnZ5VXzy/isoS1c5U0ZYUySPuTt/w2LgCPrOQY/3DHOsdoqt/mO6BEdzf/GdUlUaorSqhtrKEy+pmMLeqNHn7ja/K5K81lcXTvqwu36jARXLIwZ5T7D3SN66Qh95U1sNjb14jXVRop4t30axy1tbPelMhz03dLi0qDOBvJhNRgYuEXHf/MD9/qZNHd3aws/3EGY/NKi9KlW8p1yypOKOQx5fyjLIi7S2HkApcJIQGR+I8tecoj+3soDEaYyzhXDyvir/509Vct2wOc6tLmFNRQnFEpzvKZSpwkZCIJ5zn93fz6M4Ofrn7CP3DY8yvLuUvr1/KbWvruGTBBRypIqGkAhfJYu7OK529PL7rMI/v6uBo7zBVJRE+cPl8bltbx7qlc7T8Lo+pwEWyUMeJQR7f1cFjOzuIHu0nUmC8++K5fPmWOjZdMlcfJAqgAhfJGicHR3ki9WFkc1sPAA0XzeIrt13GzZcvYFaFjkqUM6nARQI0PBbnN3tjPLazg1/vPcZIPMGy2gr++j2ruPWqOurnlAcdUbKYClxkmiUSTsuB4zy6s4Ofv3iY3qExaipLuGt9PR9aW8fldTO0pE/SogIXmSatR/t4bFcHj+08TMeJQcqKCnn/ZckPI9+5fA6RaT7DnoRfWgVuZvcBHwcceAn4GLABuB8oBrYDf+nuY+f8Q0Ty0LHeIX76h8M8urODlw/3UlhgbFxZw+fedzHvWTOPCp0XRC7ApP96zKwOuBdY4+6DZvYQcCfw98Amd4+a2T8A/xb4XkbTipzlYM8pmtt6KI4UUBopoKy4kLKiQkpTX2XFhafvL40UUjANS+76h8f41e4jPLarg9/u6yLhcOWiGWz+4BpuuWIhtVUlGc8g+SHdt/8IUGZmo0A5MAAMu3s09fj/A76IClymwVg8wW9ejbG1+QCN0diEJ1w6l+JIAWVFb5R8wbiSLzyr/FPPKx73ZjDu/tI3vaaA/bEBHt3ZwZOvHGFoNMHi2WV8+k9WcOvaOpbXVmZug0jemrTA3b3DzO4H2oFB4EngIeB/mFmDu7cAfw4snuj1ZnYPcA9AfX39VOWWPHTk5BA/fuEgP3qhnc6TQ8yrLuHeG1dyyxULMDOGRuMMjcYZHI0zOBJnaCzB0Ejy9un7R+MMjyYYPOv+odE4x/pGGUo9Nv7PSpzHG8Ss8iJuf8dibltbx9X1M/VhpGRUOiOUWcCtwFLgBPAT4C7gDuAfzayEZKlPOP929y3AFoCGhobz+FEQSa7YeHZfF1ubD/DUnmPEE84Nq2r5L392KZtWz834B3/uzkg8wdBoIlnoI3GGxuKn3wCGRxOn3zBmVxTzzhU1Ov+ITJt0Rig3AW3uHgMws0eADe7+A2Bj6r73AqsyllLyTlf/MD9pOcSDv2+nvecUcyqK+cTGZdx5bf20ro02M0oihZRECrPueogi6RR4O7DezMpJjlA2AS1mNtfdj6X2wL8AfDWDOSUPuDvNbT1sbW7nl7s7GY0765bO5j+972Led+k8SiI6fFxkvHRm4M1mtg3YQXJMspPkSOQrZnYLUAB8x91/ndGkkrNOnhrl4R2H2Np8gNdiA1SXRrh7/UXcta6eFXOrgo4nkrXMz+cj/AvU0NDgLS0t0/b9JHu5O7sOnmBrczv/9w+HGR5LsLZ+Jnetu4ibL19AWbH2tkXeYGbb3b3h7Pt1FIFMq/7hMR7f1cHW59t5pbOXiuJC/vwdi7hzXT2XLpwRdDyRUFGBy7R4+fBJftjczmM7OxgYiXPJgmq++qHLuPWqOl2lXORt0k+OZMzgSJyfvXiYrc3t7Dp4gpJIAR+8ciF3ravnqsVaIy1yoVTgMuX2Hetja3M7D28/RO/QGMtrK/jyLWv4V1cvYka5luKJTBUVuEyJ4bE4v3r5KFufP0BzWw9Fhcb7L1vAXevqWbd0tva2RTJABS4XpL37FD/8fTs/aTlI98AIi2eX8YX3r+b2hkXUVOqkTSKZpAKXt+V3r3XzncbXaIrGKCwwNq2ey13rL2LjipppOeOfiKjA5Tz97rVuHng6yvP7e6itKuGzN63kL65ZzIIZZUFHE8k7KnBJy/P7u/nmU8ninltVwuYPruEj19br6ugiAVKBy1tScYtkLxW4TGh8cdequEWykgpczvD8/m4eeKqV3+3vpraqhC/fsoY716m4RbKRClwAaN7fzTdV3CKhogLPcypukfBSgecpFbdI+KnA80zz/m4eeLqV515TcYuEnQo8T/y+rYdvPhU9Xdx/d8sa7lJxi4SaCjzHjS/umkoVt0guUYHnqImK+85r63WpMpEcogLPMSpukfyhAs8Rv2/r4YGno/x2X7K4//bmS7hr3UUqbpEcpgIPuRdeT+5xq7hF8o8KPKRU3CKSVoGb2X3AxwEHXgI+BrwT+DpQAPQDH3X3fRnKKSmHTwzyhYdf5JnWLhW3SJ6btMDNrA64F1jj7oNm9hBwB/Al4FZ332NmnwL+FvhoJsPmu8ZojM/+aCcjYwkVt4ikPUKJAGVmNgqUA4dJ7o1Xpx6fkbpPMiCecL71dCvf+nUrq+ZW8c93X83y2sqgY4lIwCYtcHfvMLP7gXZgEHjS3Z80s48DvzCzQaAXWD/R683sHuAegPr6+ikLni+6+4f57I938UxrFx++uo6v3na59rpFBEjOr9+Smc0CbgWWAguBCjO7G7gP+IC7LwK+D3xjote7+xZ3b3D3htra2qlLnge2H+jh5m89S3NbD1/78OX8z9uvVHmLyGnpjFBuAtrcPQZgZo+Q/ADzSndvTj3nx8AvMxMx/7g733u2ja89sZeFM8t45JMbuKxuRtCxRCTLpFPg7cB6MysnOULZBLQAt5vZKnePAu8B9mQuZv7oHRrlC9te5IndR3jvmnl8/fYrmVFWFHQsEclC6czAm81sG7ADGAN2AluAQ8DDZpYAjgP/LpNB88Erh3v51NbtHDw+yH/+wCV8fONSzCzoWCKSpdJaheLum4HNZ939aOpLpsBDLQf5u8d2M6OsiB/ds55rlswOOpKIZDkdiRmwodE4X358Nw+1HGLD8jk8cMdaaqtKgo4lIiGgAg9QW9cAn/zBdvYe6ePeG1fwmZtWUVigkYmIpEcFHpAnXurkc9teJFJofP9j1/AnF88NOpKIhIwKfJqNxhN87Ym9fO/ZNq5cPJN/vutq6maWBR1LREJIBT6NOk8O8ukf7mT7geN8dMMSvvSBSyiOTHoslYjIhFTg0+SZ1hif+dEuhkfj/K8713LLFQuDjiQiIacCz7B4wvn2r1t54OlWVs6t5Dt3v0MnohKRKaECz6AzTkS1to6vfOgyyou1yUVkaqhNMmT7geN8+oc76B4Y4b99+HLuuGaxjqoUkSmlAp9i7s73f/s6//UXe3QiKhHJKBX4FOobGuXzqRNRvWfNPO7XiahEJINU4FNkT2cvn9q6g/aeU3zpA6v5xMZlGpmISEapwKfAT1oO8repE1E9+In1XLtUJ6ISkcxTgV8AnYhKRIKkAn+bXu8a4JNbd7Cns5e/unEFn9WJqERkmqnA34Zf7u7kcz95kUKdiEpEAqQCPw+j8QT//Ym9fDd1Iqp/unMti2aVBx1LRPKUCvw8fPIHO3hqz1GdiEpEsoIKPE3Heod4as9RPvXu5Xz+/auDjiMignYh09TU2gXAzVcsCDiJiEiSCjxNTdEYtVUlrFlQHXQUERFABZ6WeMJ5pjXGxpU1OrpSRLKGCjwNuztOcvzUKO9aVRt0FBGR09IqcDO7z8xeNrPdZvagmZWa2TNmtiv1ddjMHst02KA0RmOYwcaVKnARyR6TrkIxszrgXmCNuw+a2UPAHe6+cdxzHgYez1zMYDVFY1xRN4PZFcVBRxEROS3dEUoEKDOzCFAOHH7jATOrAm4EcnIP/OTgKDsPnuAGjU9EJMtMWuDu3gHcD7QDncBJd39y3FM+BDzt7r0Tvd7M7jGzFjNricViU5F5Wj23r4t4wjX/FpGsM2mBm9ks4FZgKbAQqDCzu8c95SPAg+d6vbtvcfcGd2+orQ1fCTZGY1SVRrhq8cygo4iInCGdEcpNQJu7x9x9FHgE2ABgZnOAa4GfZy5icNydxmiM61fUECnUgh0RyS7ptFI7sN7Myi25CHoTsCf12O3Az9x9KFMBg7TvWD+dJ4c0/xaRrJTODLwZ2AbsAF5KvWZL6uE7eIvxSdg1RpMzexW4iGSjtE5m5e6bgc0T3P/uqQ6UTRqjMVbMraRuZlnQUURE3kSD3XMYHInT3Naj1ScikrVU4OfQ3NbNyFhCBS4iWUsFfg6N0RglkQJdYV5EspYK/ByaojHWL5tDaVFh0FFERCakAp/AoeOneC02oNUnIpLVVOATaIomr76j+beIZDMV+AQao8eom1nG8tqKoKOIiJyTCvwso/EEz+3r5oZVtbr6johkNRX4WXa2n6BveIx3raoJOoqIyFtSgZ+lMXqMwgJjwwoVuIhkNxX4WZqiXVxdP5Pq0qKgo4iIvCUV+Dhd/cO81HFSq09EJBRU4OM825pcPqj13yISBirwcRqjMWZXFHPZwhlBRxERmZQKPCWRcJ5pjbFxZQ0FBVo+KCLZTwWe8kpnL139I5p/i0hoqMBT3rj6zsaVKnARCQcVeEpjNMalC6uprSoJOoqISFpU4EDf0Cg7DhzX+EREQkUFDjz3WjdjCdfyQREJFRU4yYs3VJZEuLp+VtBRRETSlvcF7u40RmNct3wOxZG83xwiEiJ531htXQMcOj6o+beIhE5aBW5m95nZy2a228weNLNSS/qqmUXNbI+Z3ZvpsJnwxvJBFbiIhE1ksieYWR1wL7DG3QfN7CHgDsCAxcBqd0+Y2dzMRs2MxmiMZTUVLJ5dHnQUEZHzku4IJQKUmVkEKAcOA58E/sHdEwDufiwzETNnaDTO8/u7tfpEREJp0gJ39w7gfqAd6AROuvuTwHLgL8ysxcyeMLOVE73ezO5JPaclFotNZfYL9sLrPQyNJjQ+EZFQmrTAzWwWcCuwFFgIVJjZ3UAJMOTuDcD/Bv5lote7+xZ3b3D3htra7CrKpmiM4sIC1i2bHXQUEZHzls4I5Sagzd1j7j4KPAJsAA4BD6ee8yhwRWYiZk5jNMa1S2dTXjzpRwEiIlknnQJvB9abWbklL9O+CdgDPAbcmHrOu4BoZiJmRufJQaJH+7lBFy8WkZCadNfT3ZvNbBuwAxgDdgJbgDJgq5ndB/QDH89k0KnWdHr5YCgXz4iITF7gAO6+Gdh81t3DwM1TnmiaNEW7mF9dyqp5lUFHERF5W/LySMyxeIJnWmPcsKqG5FRIRCR88rLA/3DoJL1DYxqfiEio5WWBN0ZjFBhcv0IfYIpIeOVlgTdFY1y1eCYzyouCjiIi8rblXYEfHxjhD4dO6PB5EQm9vCvwZ/Z14a6zD4pI+OVdgTdFY8wsL+KKRTODjiIickHyqsDdnaZojOtX1FBYoOWDIhJueVXge4/0caxvWPNvEckJeVXguvqOiOSSvCrwpmiM1fOrmFddGnQUEZELljcFPjA8xguv92jvW0RyRt4U+PP7uxmNu+bfIpIz8qbAG6MxyooKaVgyK+goIiJTIm8KvCka47rlcyiJFAYdRURkSuRFgR/oHuD17lOaf4tITsmLAm/S8kERyUF5UeCN0Rj1s8tZUlMRdBQRkSmT8wU+Mpbgude6tfctIjkn5wu85UAPp0biWj4oIjkn5wu8MRqjqNC4bvmcoKOIiEypnC/wpmgX77hoFpUlkaCjiIhMqZwu8GO9Q+zp7NXFi0UkJ6VV4GZ2n5m9bGa7zexBMys1s/9jZm1mtiv1dVWmw56vptYuAG5YpYsXi0jumXSuYGZ1wL3AGncfNLOHgDtSD3/O3bdlMuCFaIzGqK0qYc2C6qCjiIhMuXRHKBGgzMwiQDlwOHORpkY84TzbGmPjyhrMdPUdEck9kxa4u3cA9wPtQCdw0t2fTD38VTN70cz+0cxKJnq9md1jZi1m1hKLxaYs+GRe6jjJ8VOjWv8tIjlr0gI3s1nArcBSYCFQYWZ3A18EVgPXALOBL0z0enff4u4N7t5QWzt9ZdoUjWEGG1eqwEUkN6UzQrkJaHP3mLuPAo8AG9y905OGge8D12Yy6PlqjMa4om4GsyuKg44iIpIR6RR4O7DezMotOUzeBOwxswUAqftuA3ZnLub5OXlqlJ3tx3X0pYjktElXobh7s5ltA3YAY8BOYAvwhJnVAgbsAv5DJoOej9++1kXCdfZBEcltaR2e6O6bgc1n3X3j1MeZGo2vxqgqjXDV4plBRxERyZicOxLT3WlqjXH9ihoihTn31xMROS3nGq71WD+dJ4c0PhGRnJdzBf7G1Xf0AaaI5LqcK/DGaIyVcytZOLMs6CgiIhmVUwU+OBKnua1He98ikhdyqsCfb+tmZCyh+beI5IWcKvCmaIySSAHXLp0ddBQRkYzLqQJvjMZYv2wOpUWFQUcREcm4nCnwgz2n2B8b0PxbRPJGzhR4U2ty+aDm3yKSL3KnwKMx6maWsby2IugoIiLTIicKfDSe4Lf7urlhVa2uviMieSMnCnzHgeP0D4/xLl28WETySE4UeFNrjMICY8MKFbiI5I+cKPDGaIx31M+iurQo6CgiItMm9AXe1T/M7o5ebtD4RETyTOgL/JnTywfnBpxERGR6hb7Am6JdzKko5tKF1UFHERGZVqEu8ETCaYrG2LiyhoICLR8UkfwS6gJ/pbOX7oERHT4vInkp1AXemLr6zsaVKnARyT+hL/BLF1ZTW1USdBQRkWkX2gLvGxplx4HjOnmViOSttArczO4zs5fNbLeZPWhmpeMe+7aZ9Wcu4sSee62bsYRr/i0ieWvSAjezOuBeoMHdLwMKgTtSjzUAMzOa8BwaozEqSyJcXT8riG8vIhK4dEcoEaDMzCJAOXDYzAqBrwOfz1S4c3F3Gl+Ncd3yORRHQjsFEhG5IJO2n7t3APcD7UAncNLdnwQ+DfzU3Tvf6vVmdo+ZtZhZSywWm4rM7O8aoOPEoObfIpLX0hmhzAJuBZYCC4EKM/s3wO3Atyd7vbtvcfcGd2+orZ2awm18VVffERGJpPGcm4A2d48BmNkjwN8DZcC+1AUUys1sn7uvyFjScZpaYyyrqWDx7PLp+HYiIlkpnQFyO7DezMot2dabgG+4+3x3X+LuS4BT01XeQ6Nxnt/frdUnIpL30pmBNwPbgB3AS6nXbMlwrnN64fUehkYTGp+ISN5LZ4SCu28GNr/F45VTlmgSja/GKI4UsG7Z7On6liIiWSl0a/CaWmNcu2Q25cVpvfeIiOSsUBX44RODRI/2a3wiIkLICvyNq+/oA0wRkZAVeGM0xvzqUlbNm7aRu4hI1gpNgY/FEzzb2sUNq2pIrT0XEclroSnwPxw6Qe/QmC5eLCKSEpoCb3w1RoHB9Stqgo4iIpIVwlPgrV1ctXgmM8qLgo4iIpIVQlHgPQMjvHjohFafiIiME4oCf3ZfF+46+6CIyHihKPDGV2PMLC/iikWBXPxHRCQrheJ49OVzK5hbXU9hgZYPioi8IRQF/ql3T8uZakVEQiUUIxQREXkzFbiISEipwEVEQkoFLiISUipwEZGQUoGLiISUClxEJKRU4CIiIWXuPn3fzCwGHHibL68BuqYwTthpe/yRtsWZtD3OlAvb4yJ3f9PJoKa1wC+EmbW4e0PQObKFtscfaVucSdvjTLm8PTRCEREJKRW4iEhIhanAtwQdIMtoe/yRtsWZtD3OlLPbIzQzcBEROVOY9sBFRGQcFbiISEiFosDN7P1m9qqZ7TOzvwk6T1DMbLGZ/cbM9pjZy2b2maAzZQMzKzSznWb2s6CzBM3MZprZNjPbm/p3cl3QmYJiZvelfk52m9mDZlYadKaplvUFbmaFwD8BfwqsAT5iZmuCTRWYMeCv3f0SYD3wH/N4W4z3GWBP0CGyxAPAL919NXAlebpdzKwOuBdocPfLgELgjmBTTb2sL3DgWmCfu+939xHgR8CtAWcKhLt3uvuO1O/7SP5w1gWbKlhmtgi4Gfhu0FmCZmbVwA3A9wDcfcTdTwSbKlARoMzMIkA5cDjgPFMuDAVeBxwcd/sQeV5aAGa2BFgLNAebJHDfBD4PJIIOkgWWATHg+6mR0nfNrCLoUEFw9w7gfqAd6AROuvuTwaaaemEo8IkuRZ/Xax/NrBJ4GPisu/cGnScoZnYLcMzdtwedJUtEgKuB77j7WmAAyMvPjMxsFsn/qS8FFgIVZnZ3sKmmXhgK/BCweNztReTgf4XSZWZFJMt7q7s/EnSegL0T+DMze53kaO1GM/tBsJECdQg45O5v/K9sG8lCz0c3AW3uHnP3UeARYEPAmaZcGAr8BWClmS01s2KSH0T8NOBMgTAzIznf3OPu3wg6T9Dc/Yvuvsjdl5D8d/Frd8+5vax0ufsR4KCZXZy6axPwSoCRgtQOrDez8tTPzSZy8APdSNABJuPuY2b2aeBXJD9J/hd3fzngWEF5J/CvgZfMbFfqvi+5+y8CzCTZ5a+Aramdnf3AxwLOEwh3bzazbcAOkqu3dpKDh9TrUHoRkZAKwwhFREQmoAIXEQkpFbiISEipwEVEQkoFLiISUipwEZGQUoGLiITU/wddnsoFTYnyVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_vals_wnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random shuffling of data is also important\n",
    "# random_idxs = torch.randperm(train_n) \n",
    "# start_i = i*bs\n",
    "# end_i = start_i+bs\n",
    "# xb = x_train[random_idxs[start_i:end_i]].reshape(bs, 1, 28, 28)\n",
    "# yb = y_train[random_idxs[start_i:end_i]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning most important ``hyper-parameter`` for training neural networks - learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n",
    "import math\n",
    "def find_lr(net, loss_func, init_value = 1e-8, final_value=10., beta = 0.98, bs = 32):\n",
    "    num = (train_n-1)//bs + 1 # num of batches \n",
    "    mult = (final_value/init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "    avg_loss = 0.\n",
    "    best_loss = 0.\n",
    "    batch_num = 0.\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for i in range((train_n-1)//bs + 1):\n",
    "        batch_num += 1\n",
    "        start_i = i*bs\n",
    "        end_i = start_i+bs\n",
    "        xb = x_train[start_i:end_i].reshape(bs, 1, 28, 28)\n",
    "        yb = y_train[start_i:end_i]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net.forward(xb)\n",
    "        loss = loss_func(outputs, yb)\n",
    "        #Compute the smoothed loss\n",
    "        print(\"loss: \", loss.item())\n",
    "        avg_loss = beta * avg_loss + (1-beta) *loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**batch_num)\n",
    "        #Stop if the loss is exploding\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            return log_lrs, losses\n",
    "        #Record the best loss\n",
    "        if smoothed_loss < best_loss or batch_num==1:\n",
    "            best_loss = smoothed_loss\n",
    "        #Store the values\n",
    "        losses.append(smoothed_loss)\n",
    "        log_lrs.append(math.log10(lr))\n",
    "        #Do the SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #Update the lr for the next step\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "    return log_lrs, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3224499225616455\n",
      "loss:  2.299894094467163\n",
      "loss:  2.2834343910217285\n",
      "loss:  2.306600570678711\n",
      "loss:  2.3050811290740967\n",
      "loss:  2.3102734088897705\n",
      "loss:  2.3166613578796387\n",
      "loss:  2.2914466857910156\n",
      "loss:  2.301379680633545\n",
      "loss:  2.305265426635742\n",
      "loss:  2.2913200855255127\n",
      "loss:  2.309087038040161\n",
      "loss:  2.3321640491485596\n",
      "loss:  2.318732261657715\n",
      "loss:  2.3272206783294678\n",
      "loss:  2.312893867492676\n",
      "loss:  2.309516191482544\n",
      "loss:  2.2930595874786377\n",
      "loss:  2.3187897205352783\n",
      "loss:  2.3054327964782715\n",
      "loss:  2.3043930530548096\n",
      "loss:  2.3084170818328857\n",
      "loss:  2.283447504043579\n",
      "loss:  2.2995738983154297\n",
      "loss:  2.3290953636169434\n",
      "loss:  2.309528112411499\n",
      "loss:  2.308161735534668\n",
      "loss:  2.304192066192627\n",
      "loss:  2.3035473823547363\n",
      "loss:  2.2935867309570312\n",
      "loss:  2.3084452152252197\n",
      "loss:  2.2840070724487305\n",
      "loss:  2.2987632751464844\n",
      "loss:  2.311617374420166\n",
      "loss:  2.307941436767578\n",
      "loss:  2.3070828914642334\n",
      "loss:  2.2923624515533447\n",
      "loss:  2.287993907928467\n",
      "loss:  2.3138325214385986\n",
      "loss:  2.2788045406341553\n",
      "loss:  2.2984962463378906\n",
      "loss:  2.2901055812835693\n",
      "loss:  2.3042590618133545\n",
      "loss:  2.3078248500823975\n",
      "loss:  2.3399884700775146\n",
      "loss:  2.3387327194213867\n",
      "loss:  2.308840036392212\n",
      "loss:  2.304077386856079\n",
      "loss:  2.2941083908081055\n",
      "loss:  2.3161675930023193\n",
      "loss:  2.292673110961914\n",
      "loss:  2.298699140548706\n",
      "loss:  2.311807870864868\n",
      "loss:  2.3072152137756348\n",
      "loss:  2.289813756942749\n",
      "loss:  2.3083455562591553\n",
      "loss:  2.320655345916748\n",
      "loss:  2.2958192825317383\n",
      "loss:  2.2984421253204346\n",
      "loss:  2.3091835975646973\n",
      "loss:  2.3173365592956543\n",
      "loss:  2.314326047897339\n",
      "loss:  2.299851655960083\n",
      "loss:  2.305109739303589\n",
      "loss:  2.3038389682769775\n",
      "loss:  2.2772011756896973\n",
      "loss:  2.319159507751465\n",
      "loss:  2.3048582077026367\n",
      "loss:  2.3154146671295166\n",
      "loss:  2.3028810024261475\n",
      "loss:  2.300262212753296\n",
      "loss:  2.306591272354126\n",
      "loss:  2.302250385284424\n",
      "loss:  2.320441961288452\n",
      "loss:  2.3056082725524902\n",
      "loss:  2.2994463443756104\n",
      "loss:  2.293257236480713\n",
      "loss:  2.2986340522766113\n",
      "loss:  2.309277057647705\n",
      "loss:  2.316448450088501\n",
      "loss:  2.320324182510376\n",
      "loss:  2.307331085205078\n",
      "loss:  2.300750732421875\n",
      "loss:  2.2947282791137695\n",
      "loss:  2.3124325275421143\n",
      "loss:  2.2920782566070557\n",
      "loss:  2.331254243850708\n",
      "loss:  2.2844254970550537\n",
      "loss:  2.2836859226226807\n",
      "loss:  2.330188035964966\n",
      "loss:  2.305713415145874\n",
      "loss:  2.3144662380218506\n",
      "loss:  2.3091788291931152\n",
      "loss:  2.296436071395874\n",
      "loss:  2.293736219406128\n",
      "loss:  2.288182258605957\n",
      "loss:  2.308929681777954\n",
      "loss:  2.3040695190429688\n",
      "loss:  2.29451060295105\n",
      "loss:  2.298135995864868\n",
      "loss:  2.3109776973724365\n",
      "loss:  2.2802674770355225\n",
      "loss:  2.3249828815460205\n",
      "loss:  2.313375949859619\n",
      "loss:  2.2886691093444824\n",
      "loss:  2.2899510860443115\n",
      "loss:  2.3094372749328613\n",
      "loss:  2.29471755027771\n",
      "loss:  2.2997825145721436\n",
      "loss:  2.2985916137695312\n",
      "loss:  2.3100054264068604\n",
      "loss:  2.311296224594116\n",
      "loss:  2.3051278591156006\n",
      "loss:  2.3200626373291016\n",
      "loss:  2.2913036346435547\n",
      "loss:  2.3077638149261475\n",
      "loss:  2.3092281818389893\n",
      "loss:  2.286609649658203\n",
      "loss:  2.310739278793335\n",
      "loss:  2.3219830989837646\n",
      "loss:  2.3117973804473877\n",
      "loss:  2.3189454078674316\n",
      "loss:  2.3091964721679688\n",
      "loss:  2.2897379398345947\n",
      "loss:  2.290510416030884\n",
      "loss:  2.285762310028076\n",
      "loss:  2.3326847553253174\n",
      "loss:  2.312450647354126\n",
      "loss:  2.293853759765625\n",
      "loss:  2.3023366928100586\n",
      "loss:  2.289889097213745\n",
      "loss:  2.307466506958008\n",
      "loss:  2.295728921890259\n",
      "loss:  2.291541814804077\n",
      "loss:  2.314525842666626\n",
      "loss:  2.2985596656799316\n",
      "loss:  2.3159425258636475\n",
      "loss:  2.2856311798095703\n",
      "loss:  2.292686700820923\n",
      "loss:  2.305124044418335\n",
      "loss:  2.288750171661377\n",
      "loss:  2.2983510494232178\n",
      "loss:  2.2983734607696533\n",
      "loss:  2.2977020740509033\n",
      "loss:  2.2994093894958496\n",
      "loss:  2.309194326400757\n",
      "loss:  2.320636510848999\n",
      "loss:  2.294499158859253\n",
      "loss:  2.298650026321411\n",
      "loss:  2.2957146167755127\n",
      "loss:  2.307906150817871\n",
      "loss:  2.315617799758911\n",
      "loss:  2.2818667888641357\n",
      "loss:  2.327446222305298\n",
      "loss:  2.294520854949951\n",
      "loss:  2.3246512413024902\n",
      "loss:  2.292682647705078\n",
      "loss:  2.3130362033843994\n",
      "loss:  2.3236422538757324\n",
      "loss:  2.318289041519165\n",
      "loss:  2.313366413116455\n",
      "loss:  2.3067514896392822\n",
      "loss:  2.29643177986145\n",
      "loss:  2.2992944717407227\n",
      "loss:  2.307093858718872\n",
      "loss:  2.3177764415740967\n",
      "loss:  2.284492015838623\n",
      "loss:  2.296430826187134\n",
      "loss:  2.3043088912963867\n",
      "loss:  2.3162150382995605\n",
      "loss:  2.280838966369629\n",
      "loss:  2.3019378185272217\n",
      "loss:  2.3097846508026123\n",
      "loss:  2.3144843578338623\n",
      "loss:  2.303758144378662\n",
      "loss:  2.298076629638672\n",
      "loss:  2.3057057857513428\n",
      "loss:  2.3023509979248047\n",
      "loss:  2.3108575344085693\n",
      "loss:  2.313861846923828\n",
      "loss:  2.2911581993103027\n",
      "loss:  2.285618305206299\n",
      "loss:  2.309109926223755\n",
      "loss:  2.300642490386963\n",
      "loss:  2.302877902984619\n",
      "loss:  2.3034698963165283\n",
      "loss:  2.316321849822998\n",
      "loss:  2.2948176860809326\n",
      "loss:  2.289916515350342\n",
      "loss:  2.306796073913574\n",
      "loss:  2.3203186988830566\n",
      "loss:  2.294386386871338\n",
      "loss:  2.3019869327545166\n",
      "loss:  2.321657419204712\n",
      "loss:  2.325242757797241\n",
      "loss:  2.301135778427124\n",
      "loss:  2.2895359992980957\n",
      "loss:  2.288905143737793\n",
      "loss:  2.2970011234283447\n",
      "loss:  2.318047285079956\n",
      "loss:  2.3181426525115967\n",
      "loss:  2.2857654094696045\n",
      "loss:  2.3038737773895264\n",
      "loss:  2.292372941970825\n",
      "loss:  2.292344093322754\n",
      "loss:  2.297391176223755\n",
      "loss:  2.305233955383301\n",
      "loss:  2.298881769180298\n",
      "loss:  2.3070552349090576\n",
      "loss:  2.315965175628662\n",
      "loss:  2.28082275390625\n",
      "loss:  2.2853429317474365\n",
      "loss:  2.313966989517212\n",
      "loss:  2.2835724353790283\n",
      "loss:  2.297851800918579\n",
      "loss:  2.2951700687408447\n",
      "loss:  2.311777353286743\n",
      "loss:  2.312621593475342\n",
      "loss:  2.3029978275299072\n",
      "loss:  2.3024210929870605\n",
      "loss:  2.3244588375091553\n",
      "loss:  2.298510789871216\n",
      "loss:  2.2949154376983643\n",
      "loss:  2.320282459259033\n",
      "loss:  2.2985663414001465\n",
      "loss:  2.2983100414276123\n",
      "loss:  2.3168954849243164\n",
      "loss:  2.315713405609131\n",
      "loss:  2.330488920211792\n",
      "loss:  2.294203042984009\n",
      "loss:  2.289367198944092\n",
      "loss:  2.312892198562622\n",
      "loss:  2.3139476776123047\n",
      "loss:  2.2965807914733887\n",
      "loss:  2.308772087097168\n",
      "loss:  2.2958855628967285\n",
      "loss:  2.301300287246704\n",
      "loss:  2.267803907394409\n",
      "loss:  2.3048789501190186\n",
      "loss:  2.293259620666504\n",
      "loss:  2.299394130706787\n",
      "loss:  2.295631170272827\n",
      "loss:  2.3100168704986572\n",
      "loss:  2.3062546253204346\n",
      "loss:  2.3077094554901123\n",
      "loss:  2.3017518520355225\n",
      "loss:  2.3023929595947266\n",
      "loss:  2.290098190307617\n",
      "loss:  2.308795213699341\n",
      "loss:  2.2977700233459473\n",
      "loss:  2.275369644165039\n",
      "loss:  2.3040895462036133\n",
      "loss:  2.3268420696258545\n",
      "loss:  2.302305221557617\n",
      "loss:  2.328721046447754\n",
      "loss:  2.314458131790161\n",
      "loss:  2.308905839920044\n",
      "loss:  2.3048083782196045\n",
      "loss:  2.2788002490997314\n",
      "loss:  2.287997245788574\n",
      "loss:  2.3213348388671875\n",
      "loss:  2.294464349746704\n",
      "loss:  2.290131092071533\n",
      "loss:  2.3354713916778564\n",
      "loss:  2.3102076053619385\n",
      "loss:  2.3156187534332275\n",
      "loss:  2.2982630729675293\n",
      "loss:  2.29571795463562\n",
      "loss:  2.3141603469848633\n",
      "loss:  2.3027873039245605\n",
      "loss:  2.308246374130249\n",
      "loss:  2.300708055496216\n",
      "loss:  2.2800378799438477\n",
      "loss:  2.310413360595703\n",
      "loss:  2.3200995922088623\n",
      "loss:  2.3070976734161377\n",
      "loss:  2.30769944190979\n",
      "loss:  2.301055908203125\n",
      "loss:  2.3153820037841797\n",
      "loss:  2.2933874130249023\n",
      "loss:  2.311124086380005\n",
      "loss:  2.2792677879333496\n",
      "loss:  2.310089349746704\n",
      "loss:  2.2877213954925537\n",
      "loss:  2.311338424682617\n",
      "loss:  2.2909116744995117\n",
      "loss:  2.3173606395721436\n",
      "loss:  2.3009696006774902\n",
      "loss:  2.3057992458343506\n",
      "loss:  2.294182538986206\n",
      "loss:  2.3102149963378906\n",
      "loss:  2.3101727962493896\n",
      "loss:  2.306889057159424\n",
      "loss:  2.3153183460235596\n",
      "loss:  2.3159584999084473\n",
      "loss:  2.3105053901672363\n",
      "loss:  2.31599497795105\n",
      "loss:  2.312166690826416\n",
      "loss:  2.2883706092834473\n",
      "loss:  2.3024938106536865\n",
      "loss:  2.2934658527374268\n",
      "loss:  2.303956985473633\n",
      "loss:  2.3151121139526367\n",
      "loss:  2.2813122272491455\n",
      "loss:  2.3085896968841553\n",
      "loss:  2.306215763092041\n",
      "loss:  2.320601224899292\n",
      "loss:  2.3050894737243652\n",
      "loss:  2.3045456409454346\n",
      "loss:  2.283531427383423\n",
      "loss:  2.300178289413452\n",
      "loss:  2.287930727005005\n",
      "loss:  2.277761697769165\n",
      "loss:  2.3051390647888184\n",
      "loss:  2.293623447418213\n",
      "loss:  2.2964892387390137\n",
      "loss:  2.3200063705444336\n",
      "loss:  2.3062081336975098\n",
      "loss:  2.305889368057251\n",
      "loss:  2.306809902191162\n",
      "loss:  2.29961895942688\n",
      "loss:  2.2801764011383057\n",
      "loss:  2.314025640487671\n",
      "loss:  2.3093061447143555\n",
      "loss:  2.300489664077759\n",
      "loss:  2.3295609951019287\n",
      "loss:  2.299314260482788\n",
      "loss:  2.302186965942383\n",
      "loss:  2.2949161529541016\n",
      "loss:  2.302539825439453\n",
      "loss:  2.320920944213867\n",
      "loss:  2.308154344558716\n",
      "loss:  2.3388357162475586\n",
      "loss:  2.2846317291259766\n",
      "loss:  2.2894933223724365\n",
      "loss:  2.3038811683654785\n",
      "loss:  2.3189494609832764\n",
      "loss:  2.292088270187378\n",
      "loss:  2.3182520866394043\n",
      "loss:  2.2957029342651367\n",
      "loss:  2.3090980052948\n",
      "loss:  2.286651134490967\n",
      "loss:  2.297996997833252\n",
      "loss:  2.2986042499542236\n",
      "loss:  2.3043999671936035\n",
      "loss:  2.3103623390197754\n",
      "loss:  2.3093960285186768\n",
      "loss:  2.314645528793335\n",
      "loss:  2.3045589923858643\n",
      "loss:  2.2976784706115723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.296468496322632\n",
      "loss:  2.327486276626587\n",
      "loss:  2.28169584274292\n",
      "loss:  2.303955078125\n",
      "loss:  2.3086905479431152\n",
      "loss:  2.3310694694519043\n",
      "loss:  2.3062007427215576\n",
      "loss:  2.297142744064331\n",
      "loss:  2.2877068519592285\n",
      "loss:  2.3096108436584473\n",
      "loss:  2.3164825439453125\n",
      "loss:  2.3084352016448975\n",
      "loss:  2.2926204204559326\n",
      "loss:  2.3152987957000732\n",
      "loss:  2.3012356758117676\n",
      "loss:  2.281466484069824\n",
      "loss:  2.3153843879699707\n",
      "loss:  2.3029956817626953\n",
      "loss:  2.2812929153442383\n",
      "loss:  2.3318076133728027\n",
      "loss:  2.3108267784118652\n",
      "loss:  2.283231496810913\n",
      "loss:  2.314513921737671\n",
      "loss:  2.2922942638397217\n",
      "loss:  2.2945306301116943\n",
      "loss:  2.2939374446868896\n",
      "loss:  2.3080873489379883\n",
      "loss:  2.323988914489746\n",
      "loss:  2.3216447830200195\n",
      "loss:  2.314572811126709\n",
      "loss:  2.3167810440063477\n",
      "loss:  2.308154582977295\n",
      "loss:  2.312232732772827\n",
      "loss:  2.300429582595825\n",
      "loss:  2.3006632328033447\n",
      "loss:  2.3033041954040527\n",
      "loss:  2.3209452629089355\n",
      "loss:  2.3017916679382324\n",
      "loss:  2.312166452407837\n",
      "loss:  2.2811532020568848\n",
      "loss:  2.308239459991455\n",
      "loss:  2.3124489784240723\n",
      "loss:  2.3380491733551025\n",
      "loss:  2.3132412433624268\n",
      "loss:  2.3166849613189697\n",
      "loss:  2.320394992828369\n",
      "loss:  2.327538013458252\n",
      "loss:  2.3242340087890625\n",
      "loss:  2.2920618057250977\n",
      "loss:  2.2954211235046387\n",
      "loss:  2.299445629119873\n",
      "loss:  2.2978875637054443\n",
      "loss:  2.310197114944458\n",
      "loss:  2.305417537689209\n",
      "loss:  2.310476064682007\n",
      "loss:  2.3263659477233887\n",
      "loss:  2.2911715507507324\n",
      "loss:  2.297319173812866\n",
      "loss:  2.3077309131622314\n",
      "loss:  2.299149751663208\n",
      "loss:  2.3096039295196533\n",
      "loss:  2.308751344680786\n",
      "loss:  2.310736894607544\n",
      "loss:  2.3142449855804443\n",
      "loss:  2.298441171646118\n",
      "loss:  2.2950828075408936\n",
      "loss:  2.290771007537842\n",
      "loss:  2.303880453109741\n",
      "loss:  2.310926675796509\n",
      "loss:  2.2973251342773438\n",
      "loss:  2.293041229248047\n",
      "loss:  2.2980551719665527\n",
      "loss:  2.3118743896484375\n",
      "loss:  2.298398971557617\n",
      "loss:  2.2882256507873535\n",
      "loss:  2.3034658432006836\n",
      "loss:  2.3219425678253174\n",
      "loss:  2.320516586303711\n",
      "loss:  2.322972536087036\n",
      "loss:  2.3047034740448\n",
      "loss:  2.2898664474487305\n",
      "loss:  2.3115739822387695\n",
      "loss:  2.3000028133392334\n",
      "loss:  2.289898157119751\n",
      "loss:  2.3305046558380127\n",
      "loss:  2.3185548782348633\n",
      "loss:  2.294158935546875\n",
      "loss:  2.296133518218994\n",
      "loss:  2.292393207550049\n",
      "loss:  2.3103349208831787\n",
      "loss:  2.305231809616089\n",
      "loss:  2.323972225189209\n",
      "loss:  2.3107712268829346\n",
      "loss:  2.2888708114624023\n",
      "loss:  2.3142740726470947\n",
      "loss:  2.280808687210083\n",
      "loss:  2.293858051300049\n",
      "loss:  2.2966105937957764\n",
      "loss:  2.308926820755005\n",
      "loss:  2.3030145168304443\n",
      "loss:  2.287567377090454\n",
      "loss:  2.305546522140503\n",
      "loss:  2.2947421073913574\n",
      "loss:  2.3169326782226562\n",
      "loss:  2.30190110206604\n",
      "loss:  2.2946584224700928\n",
      "loss:  2.3061864376068115\n",
      "loss:  2.3048861026763916\n",
      "loss:  2.313539505004883\n",
      "loss:  2.3149101734161377\n",
      "loss:  2.2878265380859375\n",
      "loss:  2.304572820663452\n",
      "loss:  2.2985267639160156\n",
      "loss:  2.3227012157440186\n",
      "loss:  2.3052003383636475\n",
      "loss:  2.316044807434082\n",
      "loss:  2.316774368286133\n",
      "loss:  2.3190455436706543\n",
      "loss:  2.2892673015594482\n",
      "loss:  2.2981367111206055\n",
      "loss:  2.2926387786865234\n",
      "loss:  2.326691150665283\n",
      "loss:  2.2893152236938477\n",
      "loss:  2.3094675540924072\n",
      "loss:  2.3144309520721436\n",
      "loss:  2.3082289695739746\n",
      "loss:  2.307633399963379\n",
      "loss:  2.291311264038086\n",
      "loss:  2.313912868499756\n",
      "loss:  2.3031809329986572\n",
      "loss:  2.2867343425750732\n",
      "loss:  2.297633171081543\n",
      "loss:  2.295586585998535\n",
      "loss:  2.3123562335968018\n",
      "loss:  2.3046982288360596\n",
      "loss:  2.3179636001586914\n",
      "loss:  2.313262701034546\n",
      "loss:  2.3010973930358887\n",
      "loss:  2.3020763397216797\n",
      "loss:  2.2969768047332764\n",
      "loss:  2.3265154361724854\n",
      "loss:  2.3110697269439697\n",
      "loss:  2.311398983001709\n",
      "loss:  2.3230700492858887\n",
      "loss:  2.2969110012054443\n",
      "loss:  2.322861671447754\n",
      "loss:  2.308535575866699\n",
      "loss:  2.289720296859741\n",
      "loss:  2.3418610095977783\n",
      "loss:  2.2908382415771484\n",
      "loss:  2.3271377086639404\n",
      "loss:  2.3118066787719727\n",
      "loss:  2.3005025386810303\n",
      "loss:  2.2986395359039307\n",
      "loss:  2.3056604862213135\n",
      "loss:  2.282650947570801\n",
      "loss:  2.2914884090423584\n",
      "loss:  2.316296339035034\n",
      "loss:  2.306037664413452\n",
      "loss:  2.3078973293304443\n",
      "loss:  2.2938075065612793\n",
      "loss:  2.325362205505371\n",
      "loss:  2.2867825031280518\n",
      "loss:  2.2978267669677734\n",
      "loss:  2.291604995727539\n",
      "loss:  2.297348976135254\n",
      "loss:  2.2965760231018066\n",
      "loss:  2.312791347503662\n",
      "loss:  2.3138601779937744\n",
      "loss:  2.2694365978240967\n",
      "loss:  2.306391716003418\n",
      "loss:  2.2905917167663574\n",
      "loss:  2.3238277435302734\n",
      "loss:  2.3058338165283203\n",
      "loss:  2.296797513961792\n",
      "loss:  2.2997469902038574\n",
      "loss:  2.304222345352173\n",
      "loss:  2.290931463241577\n",
      "loss:  2.3276140689849854\n",
      "loss:  2.2960424423217773\n",
      "loss:  2.323108196258545\n",
      "loss:  2.313319206237793\n",
      "loss:  2.311506748199463\n",
      "loss:  2.313927412033081\n",
      "loss:  2.3032164573669434\n",
      "loss:  2.292283773422241\n",
      "loss:  2.3093039989471436\n",
      "loss:  2.3008971214294434\n",
      "loss:  2.3017523288726807\n",
      "loss:  2.3281350135803223\n",
      "loss:  2.3010973930358887\n",
      "loss:  2.2861390113830566\n",
      "loss:  2.2986199855804443\n",
      "loss:  2.2994747161865234\n",
      "loss:  2.300204038619995\n",
      "loss:  2.306795835494995\n",
      "loss:  2.2748804092407227\n",
      "loss:  2.302675485610962\n",
      "loss:  2.309774398803711\n",
      "loss:  2.2868998050689697\n",
      "loss:  2.290605068206787\n",
      "loss:  2.2817747592926025\n",
      "loss:  2.290825843811035\n",
      "loss:  2.317518949508667\n",
      "loss:  2.301924467086792\n",
      "loss:  2.2999749183654785\n",
      "loss:  2.311972141265869\n",
      "loss:  2.2847440242767334\n",
      "loss:  2.305624485015869\n",
      "loss:  2.3041391372680664\n",
      "loss:  2.309014081954956\n",
      "loss:  2.3043622970581055\n",
      "loss:  2.2942698001861572\n",
      "loss:  2.33858323097229\n",
      "loss:  2.299809455871582\n",
      "loss:  2.32300066947937\n",
      "loss:  2.3144116401672363\n",
      "loss:  2.2861926555633545\n",
      "loss:  2.3089025020599365\n",
      "loss:  2.296550750732422\n",
      "loss:  2.3149025440216064\n",
      "loss:  2.2995645999908447\n",
      "loss:  2.3034250736236572\n",
      "loss:  2.283377170562744\n",
      "loss:  2.292701482772827\n",
      "loss:  2.317683458328247\n",
      "loss:  2.328277587890625\n",
      "loss:  2.31119704246521\n",
      "loss:  2.2942380905151367\n",
      "loss:  2.275916814804077\n",
      "loss:  2.303539276123047\n",
      "loss:  2.3116493225097656\n",
      "loss:  2.2966115474700928\n",
      "loss:  2.312880754470825\n",
      "loss:  2.312760591506958\n",
      "loss:  2.3095860481262207\n",
      "loss:  2.292156457901001\n",
      "loss:  2.31801700592041\n",
      "loss:  2.306694984436035\n",
      "loss:  2.3037474155426025\n",
      "loss:  2.3014121055603027\n",
      "loss:  2.317288875579834\n",
      "loss:  2.299633264541626\n",
      "loss:  2.306635856628418\n",
      "loss:  2.2874720096588135\n",
      "loss:  2.2871041297912598\n",
      "loss:  2.311241865158081\n",
      "loss:  2.2933349609375\n",
      "loss:  2.322833299636841\n",
      "loss:  2.311575412750244\n",
      "loss:  2.308910608291626\n",
      "loss:  2.29267954826355\n",
      "loss:  2.314116954803467\n",
      "loss:  2.301799774169922\n",
      "loss:  2.3075971603393555\n",
      "loss:  2.3085386753082275\n",
      "loss:  2.2992568016052246\n",
      "loss:  2.297182559967041\n",
      "loss:  2.29756498336792\n",
      "loss:  2.3261821269989014\n",
      "loss:  2.304156541824341\n",
      "loss:  2.2959792613983154\n",
      "loss:  2.294318437576294\n",
      "loss:  2.3208401203155518\n",
      "loss:  2.311668634414673\n",
      "loss:  2.3011343479156494\n",
      "loss:  2.304168701171875\n",
      "loss:  2.3042356967926025\n",
      "loss:  2.312042474746704\n",
      "loss:  2.289665699005127\n",
      "loss:  2.322648048400879\n",
      "loss:  2.2999658584594727\n",
      "loss:  2.309551477432251\n",
      "loss:  2.3252174854278564\n",
      "loss:  2.2995898723602295\n",
      "loss:  2.300013780593872\n",
      "loss:  2.3064866065979004\n",
      "loss:  2.314309597015381\n",
      "loss:  2.2839834690093994\n",
      "loss:  2.291868209838867\n",
      "loss:  2.278441905975342\n",
      "loss:  2.3094401359558105\n",
      "loss:  2.2882797718048096\n",
      "loss:  2.296055793762207\n",
      "loss:  2.3162434101104736\n",
      "loss:  2.3006439208984375\n",
      "loss:  2.313077926635742\n",
      "loss:  2.284349203109741\n",
      "loss:  2.3141770362854004\n",
      "loss:  2.3145370483398438\n",
      "loss:  2.306321144104004\n",
      "loss:  2.3022778034210205\n",
      "loss:  2.287818670272827\n",
      "loss:  2.321240186691284\n",
      "loss:  2.310711145401001\n",
      "loss:  2.3070952892303467\n",
      "loss:  2.297292470932007\n",
      "loss:  2.307713508605957\n",
      "loss:  2.3158698081970215\n",
      "loss:  2.285362958908081\n",
      "loss:  2.2962379455566406\n",
      "loss:  2.3059797286987305\n",
      "loss:  2.3027305603027344\n",
      "loss:  2.2863142490386963\n",
      "loss:  2.307112216949463\n",
      "loss:  2.313960075378418\n",
      "loss:  2.3040783405303955\n",
      "loss:  2.291322946548462\n",
      "loss:  2.2817187309265137\n",
      "loss:  2.305142402648926\n",
      "loss:  2.311479330062866\n",
      "loss:  2.3061110973358154\n",
      "loss:  2.2734899520874023\n",
      "loss:  2.2778661251068115\n",
      "loss:  2.3155202865600586\n",
      "loss:  2.305785894393921\n",
      "loss:  2.3150646686553955\n",
      "loss:  2.3156869411468506\n",
      "loss:  2.3007426261901855\n",
      "loss:  2.2985239028930664\n",
      "loss:  2.311720609664917\n",
      "loss:  2.309718370437622\n",
      "loss:  2.2893216609954834\n",
      "loss:  2.3128716945648193\n",
      "loss:  2.300374984741211\n",
      "loss:  2.3244078159332275\n",
      "loss:  2.3021886348724365\n",
      "loss:  2.305812358856201\n",
      "loss:  2.296459197998047\n",
      "loss:  2.301908254623413\n",
      "loss:  2.2943718433380127\n",
      "loss:  2.3003761768341064\n",
      "loss:  2.307528257369995\n",
      "loss:  2.3067588806152344\n",
      "loss:  2.3095927238464355\n",
      "loss:  2.319333791732788\n",
      "loss:  2.2777655124664307\n",
      "loss:  2.297266960144043\n",
      "loss:  2.3164329528808594\n",
      "loss:  2.298795223236084\n",
      "loss:  2.2911620140075684\n",
      "loss:  2.2929201126098633\n",
      "loss:  2.291825294494629\n",
      "loss:  2.2983956336975098\n",
      "loss:  2.292910575866699\n",
      "loss:  2.3065006732940674\n",
      "loss:  2.30372953414917\n",
      "loss:  2.2914042472839355\n",
      "loss:  2.266376495361328\n",
      "loss:  2.2954366207122803\n",
      "loss:  2.3217384815216064\n",
      "loss:  2.2907462120056152\n",
      "loss:  2.338193655014038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.306889772415161\n",
      "loss:  2.29433012008667\n",
      "loss:  2.3025906085968018\n",
      "loss:  2.3168742656707764\n",
      "loss:  2.296738386154175\n",
      "loss:  2.3082592487335205\n",
      "loss:  2.3002750873565674\n",
      "loss:  2.294738292694092\n",
      "loss:  2.3091557025909424\n",
      "loss:  2.2952091693878174\n",
      "loss:  2.2983310222625732\n",
      "loss:  2.2928738594055176\n",
      "loss:  2.3059027194976807\n",
      "loss:  2.3301608562469482\n",
      "loss:  2.3048219680786133\n",
      "loss:  2.295713186264038\n",
      "loss:  2.2816812992095947\n",
      "loss:  2.315260410308838\n",
      "loss:  2.307384490966797\n",
      "loss:  2.3070812225341797\n",
      "loss:  2.3118395805358887\n",
      "loss:  2.2787859439849854\n",
      "loss:  2.3068854808807373\n",
      "loss:  2.3036837577819824\n",
      "loss:  2.3135433197021484\n",
      "loss:  2.310903310775757\n",
      "loss:  2.3100905418395996\n",
      "loss:  2.3093862533569336\n",
      "loss:  2.2861886024475098\n",
      "loss:  2.289072275161743\n",
      "loss:  2.2960851192474365\n",
      "loss:  2.3116402626037598\n",
      "loss:  2.325544595718384\n",
      "loss:  2.309683322906494\n",
      "loss:  2.3083651065826416\n",
      "loss:  2.2963762283325195\n",
      "loss:  2.3256118297576904\n",
      "loss:  2.321362257003784\n",
      "loss:  2.2895236015319824\n",
      "loss:  2.3100433349609375\n",
      "loss:  2.3078432083129883\n",
      "loss:  2.323065757751465\n",
      "loss:  2.2908358573913574\n",
      "loss:  2.3020522594451904\n",
      "loss:  2.297016143798828\n",
      "loss:  2.2868168354034424\n",
      "loss:  2.284529685974121\n",
      "loss:  2.2859466075897217\n",
      "loss:  2.2808899879455566\n",
      "loss:  2.2823283672332764\n",
      "loss:  2.3054189682006836\n",
      "loss:  2.295792818069458\n",
      "loss:  2.302279472351074\n",
      "loss:  2.295875310897827\n",
      "loss:  2.3177103996276855\n",
      "loss:  2.2976253032684326\n",
      "loss:  2.310713529586792\n",
      "loss:  2.2966880798339844\n",
      "loss:  2.304234027862549\n",
      "loss:  2.2913711071014404\n",
      "loss:  2.309596061706543\n",
      "loss:  2.3011176586151123\n",
      "loss:  2.296457529067993\n",
      "loss:  2.2999074459075928\n",
      "loss:  2.3067266941070557\n",
      "loss:  2.3125481605529785\n",
      "loss:  2.2965638637542725\n",
      "loss:  2.30116868019104\n",
      "loss:  2.3030807971954346\n",
      "loss:  2.3025968074798584\n",
      "loss:  2.302459955215454\n",
      "loss:  2.317293882369995\n",
      "loss:  2.272066831588745\n",
      "loss:  2.29134464263916\n",
      "loss:  2.3077402114868164\n",
      "loss:  2.3209774494171143\n",
      "loss:  2.2938222885131836\n",
      "loss:  2.313559055328369\n",
      "loss:  2.3087282180786133\n",
      "loss:  2.280149459838867\n",
      "loss:  2.3014235496520996\n",
      "loss:  2.280488967895508\n",
      "loss:  2.28057861328125\n",
      "loss:  2.3089027404785156\n",
      "loss:  2.3005740642547607\n",
      "loss:  2.3126819133758545\n",
      "loss:  2.304590940475464\n",
      "loss:  2.293907642364502\n",
      "loss:  2.302311420440674\n",
      "loss:  2.29791522026062\n",
      "loss:  2.3100481033325195\n",
      "loss:  2.3048627376556396\n",
      "loss:  2.2905592918395996\n",
      "loss:  2.32334041595459\n",
      "loss:  2.300480604171753\n",
      "loss:  2.3013360500335693\n",
      "loss:  2.315732479095459\n",
      "loss:  2.2917354106903076\n",
      "loss:  2.3187096118927\n",
      "loss:  2.309786319732666\n",
      "loss:  2.3039653301239014\n",
      "loss:  2.3084778785705566\n",
      "loss:  2.31801438331604\n",
      "loss:  2.303219795227051\n",
      "loss:  2.288527727127075\n",
      "loss:  2.2951416969299316\n",
      "loss:  2.317312002182007\n",
      "loss:  2.283365488052368\n",
      "loss:  2.319162130355835\n",
      "loss:  2.285888433456421\n",
      "loss:  2.2875280380249023\n",
      "loss:  2.3078603744506836\n",
      "loss:  2.2893054485321045\n",
      "loss:  2.291945457458496\n",
      "loss:  2.3037266731262207\n",
      "loss:  2.32951021194458\n",
      "loss:  2.297774076461792\n",
      "loss:  2.283222198486328\n",
      "loss:  2.2985870838165283\n",
      "loss:  2.2989978790283203\n",
      "loss:  2.2975316047668457\n",
      "loss:  2.291123390197754\n",
      "loss:  2.2931408882141113\n",
      "loss:  2.3025341033935547\n",
      "loss:  2.2981979846954346\n",
      "loss:  2.3119418621063232\n",
      "loss:  2.3153369426727295\n",
      "loss:  2.305299997329712\n",
      "loss:  2.3214852809906006\n",
      "loss:  2.2939906120300293\n",
      "loss:  2.30755877494812\n",
      "loss:  2.3042869567871094\n",
      "loss:  2.2966079711914062\n",
      "loss:  2.3034584522247314\n",
      "loss:  2.2765071392059326\n",
      "loss:  2.309821605682373\n",
      "loss:  2.279278516769409\n",
      "loss:  2.300274133682251\n",
      "loss:  2.3138298988342285\n",
      "loss:  2.295865774154663\n",
      "loss:  2.3031818866729736\n",
      "loss:  2.3260347843170166\n",
      "loss:  2.3041598796844482\n",
      "loss:  2.302335739135742\n",
      "loss:  2.290832281112671\n",
      "loss:  2.3096978664398193\n",
      "loss:  2.298736095428467\n",
      "loss:  2.3113760948181152\n",
      "loss:  2.3229594230651855\n",
      "loss:  2.2991068363189697\n",
      "loss:  2.3016881942749023\n",
      "loss:  2.298922538757324\n",
      "loss:  2.3034303188323975\n",
      "loss:  2.315109968185425\n",
      "loss:  2.2771427631378174\n",
      "loss:  2.299980401992798\n",
      "loss:  2.3001441955566406\n",
      "loss:  2.2945494651794434\n",
      "loss:  2.2932913303375244\n",
      "loss:  2.2765328884124756\n",
      "loss:  2.3080191612243652\n",
      "loss:  2.3160810470581055\n",
      "loss:  2.3072195053100586\n",
      "loss:  2.268305778503418\n",
      "loss:  2.3125009536743164\n",
      "loss:  2.3174121379852295\n",
      "loss:  2.288975954055786\n",
      "loss:  2.307368516921997\n",
      "loss:  2.2886786460876465\n",
      "loss:  2.319340944290161\n",
      "loss:  2.301722764968872\n",
      "loss:  2.2733607292175293\n",
      "loss:  2.3002889156341553\n",
      "loss:  2.317272424697876\n",
      "loss:  2.3183114528656006\n",
      "loss:  2.299107551574707\n",
      "loss:  2.317298173904419\n",
      "loss:  2.3145904541015625\n",
      "loss:  2.301476001739502\n",
      "loss:  2.300450325012207\n",
      "loss:  2.2810275554656982\n",
      "loss:  2.308577299118042\n",
      "loss:  2.311830759048462\n",
      "loss:  2.3116066455841064\n",
      "loss:  2.306535482406616\n",
      "loss:  2.3037021160125732\n",
      "loss:  2.3084559440612793\n",
      "loss:  2.3154518604278564\n",
      "loss:  2.3045647144317627\n",
      "loss:  2.3120574951171875\n",
      "loss:  2.2970924377441406\n",
      "loss:  2.297665596008301\n",
      "loss:  2.305830955505371\n",
      "loss:  2.2942514419555664\n",
      "loss:  2.309757947921753\n",
      "loss:  2.317382335662842\n",
      "loss:  2.2907135486602783\n",
      "loss:  2.3201916217803955\n",
      "loss:  2.3035337924957275\n",
      "loss:  2.335232734680176\n",
      "loss:  2.2987325191497803\n",
      "loss:  2.2962779998779297\n",
      "loss:  2.3363354206085205\n",
      "loss:  2.3131368160247803\n",
      "loss:  2.299431324005127\n",
      "loss:  2.3054070472717285\n",
      "loss:  2.288468837738037\n",
      "loss:  2.302696704864502\n",
      "loss:  2.2953526973724365\n",
      "loss:  2.3233678340911865\n",
      "loss:  2.321291446685791\n",
      "loss:  2.324322462081909\n",
      "loss:  2.290424108505249\n",
      "loss:  2.2924084663391113\n",
      "loss:  2.315589666366577\n",
      "loss:  2.3214311599731445\n",
      "loss:  2.3076539039611816\n",
      "loss:  2.3140785694122314\n",
      "loss:  2.3177261352539062\n",
      "loss:  2.2918622493743896\n",
      "loss:  2.3143372535705566\n",
      "loss:  2.287174701690674\n",
      "loss:  2.3045437335968018\n",
      "loss:  2.3347256183624268\n",
      "loss:  2.310330390930176\n",
      "loss:  2.318197011947632\n",
      "loss:  2.2905514240264893\n",
      "loss:  2.2959370613098145\n",
      "loss:  2.2840752601623535\n",
      "loss:  2.2922799587249756\n",
      "loss:  2.306161642074585\n",
      "loss:  2.3008346557617188\n",
      "loss:  2.2970516681671143\n",
      "loss:  2.290215015411377\n",
      "loss:  2.3128209114074707\n",
      "loss:  2.2938966751098633\n",
      "loss:  2.291475772857666\n",
      "loss:  2.317857503890991\n",
      "loss:  2.3161909580230713\n",
      "loss:  2.3024954795837402\n",
      "loss:  2.3050975799560547\n",
      "loss:  2.3389599323272705\n",
      "loss:  2.311485767364502\n",
      "loss:  2.298107147216797\n",
      "loss:  2.2849340438842773\n",
      "loss:  2.3175017833709717\n",
      "loss:  2.305117607116699\n",
      "loss:  2.276773452758789\n",
      "loss:  2.303109645843506\n",
      "loss:  2.2863285541534424\n",
      "loss:  2.3092422485351562\n",
      "loss:  2.306767463684082\n",
      "loss:  2.3030333518981934\n",
      "loss:  2.3134729862213135\n",
      "loss:  2.283726930618286\n",
      "loss:  2.294485569000244\n",
      "loss:  2.322746753692627\n",
      "loss:  2.3013553619384766\n",
      "loss:  2.295804977416992\n",
      "loss:  2.296631336212158\n",
      "loss:  2.308872938156128\n",
      "loss:  2.30855131149292\n",
      "loss:  2.2642226219177246\n",
      "loss:  2.300503969192505\n",
      "loss:  2.2916011810302734\n",
      "loss:  2.3013203144073486\n",
      "loss:  2.329332113265991\n",
      "loss:  2.310716390609741\n",
      "loss:  2.3037662506103516\n",
      "loss:  2.3342368602752686\n",
      "loss:  2.3176820278167725\n",
      "loss:  2.3004422187805176\n",
      "loss:  2.304398775100708\n",
      "loss:  2.3010380268096924\n",
      "loss:  2.315223455429077\n",
      "loss:  2.3104565143585205\n",
      "loss:  2.2969017028808594\n",
      "loss:  2.3006234169006348\n",
      "loss:  2.2957582473754883\n",
      "loss:  2.285139560699463\n",
      "loss:  2.300851821899414\n",
      "loss:  2.3003885746002197\n",
      "loss:  2.294537305831909\n",
      "loss:  2.2948503494262695\n",
      "loss:  2.3038196563720703\n",
      "loss:  2.314077377319336\n",
      "loss:  2.310774087905884\n",
      "loss:  2.3042073249816895\n",
      "loss:  2.29276704788208\n",
      "loss:  2.2966463565826416\n",
      "loss:  2.2912142276763916\n",
      "loss:  2.305223226547241\n",
      "loss:  2.3130393028259277\n",
      "loss:  2.3001372814178467\n",
      "loss:  2.315981864929199\n",
      "loss:  2.3129818439483643\n",
      "loss:  2.3131349086761475\n",
      "loss:  2.2970876693725586\n",
      "loss:  2.303112268447876\n",
      "loss:  2.2861294746398926\n",
      "loss:  2.3117642402648926\n",
      "loss:  2.2904245853424072\n",
      "loss:  2.3121330738067627\n",
      "loss:  2.3050119876861572\n",
      "loss:  2.302752733230591\n",
      "loss:  2.311129093170166\n",
      "loss:  2.303868055343628\n",
      "loss:  2.273502826690674\n",
      "loss:  2.3099184036254883\n",
      "loss:  2.304980993270874\n",
      "loss:  2.3058905601501465\n",
      "loss:  2.308529853820801\n",
      "loss:  2.2904529571533203\n",
      "loss:  2.299499273300171\n",
      "loss:  2.29219126701355\n",
      "loss:  2.3111095428466797\n",
      "loss:  2.297632932662964\n",
      "loss:  2.298861265182495\n",
      "loss:  2.3027114868164062\n",
      "loss:  2.2744874954223633\n",
      "loss:  2.3104379177093506\n",
      "loss:  2.3044986724853516\n",
      "loss:  2.286810874938965\n",
      "loss:  2.299177885055542\n",
      "loss:  2.313448190689087\n",
      "loss:  2.3009016513824463\n",
      "loss:  2.277071714401245\n",
      "loss:  2.2819883823394775\n",
      "loss:  2.2912375926971436\n",
      "loss:  2.3051161766052246\n",
      "loss:  2.290159225463867\n",
      "loss:  2.3124773502349854\n",
      "loss:  2.276726722717285\n",
      "loss:  2.303476095199585\n",
      "loss:  2.2824318408966064\n",
      "loss:  2.292349338531494\n",
      "loss:  2.300767183303833\n",
      "loss:  2.298740863800049\n",
      "loss:  2.294149160385132\n",
      "loss:  2.311504364013672\n",
      "loss:  2.295116424560547\n",
      "loss:  2.3312063217163086\n",
      "loss:  2.2906267642974854\n",
      "loss:  2.3195674419403076\n",
      "loss:  2.3125720024108887\n",
      "loss:  2.296603202819824\n",
      "loss:  2.300527334213257\n",
      "loss:  2.310366630554199\n",
      "loss:  2.310577869415283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.3016257286071777\n",
      "loss:  2.292759895324707\n",
      "loss:  2.3031132221221924\n",
      "loss:  2.2982099056243896\n",
      "loss:  2.313737630844116\n",
      "loss:  2.285757064819336\n",
      "loss:  2.2817652225494385\n",
      "loss:  2.2772626876831055\n",
      "loss:  2.3219659328460693\n",
      "loss:  2.3040647506713867\n",
      "loss:  2.300097703933716\n",
      "loss:  2.307608127593994\n",
      "loss:  2.305471897125244\n",
      "loss:  2.3065359592437744\n",
      "loss:  2.30871319770813\n",
      "loss:  2.3030574321746826\n",
      "loss:  2.3229660987854004\n",
      "loss:  2.298361301422119\n",
      "loss:  2.3005645275115967\n",
      "loss:  2.3105344772338867\n",
      "loss:  2.298954486846924\n",
      "loss:  2.28822922706604\n",
      "loss:  2.286405086517334\n",
      "loss:  2.2963390350341797\n",
      "loss:  2.3008511066436768\n",
      "loss:  2.2860403060913086\n",
      "loss:  2.31099009513855\n",
      "loss:  2.285759687423706\n",
      "loss:  2.3239638805389404\n",
      "loss:  2.317564010620117\n",
      "loss:  2.285729169845581\n",
      "loss:  2.2895891666412354\n",
      "loss:  2.2933356761932373\n",
      "loss:  2.3055646419525146\n",
      "loss:  2.3083112239837646\n",
      "loss:  2.3024802207946777\n",
      "loss:  2.3056771755218506\n",
      "loss:  2.2801249027252197\n",
      "loss:  2.29545521736145\n",
      "loss:  2.282653331756592\n",
      "loss:  2.322070360183716\n",
      "loss:  2.295438766479492\n",
      "loss:  2.293506145477295\n",
      "loss:  2.291102170944214\n",
      "loss:  2.2863411903381348\n",
      "loss:  2.295454740524292\n",
      "loss:  2.300771951675415\n",
      "loss:  2.2998738288879395\n",
      "loss:  2.3019423484802246\n",
      "loss:  2.331967830657959\n",
      "loss:  2.283291816711426\n",
      "loss:  2.3009746074676514\n",
      "loss:  2.305663585662842\n",
      "loss:  2.286731243133545\n",
      "loss:  2.2907705307006836\n",
      "loss:  2.296058416366577\n",
      "loss:  2.28863263130188\n",
      "loss:  2.2782890796661377\n",
      "loss:  2.3006398677825928\n",
      "loss:  2.3093793392181396\n",
      "loss:  2.3097715377807617\n",
      "loss:  2.300600051879883\n",
      "loss:  2.3012688159942627\n",
      "loss:  2.3283374309539795\n",
      "loss:  2.3147785663604736\n",
      "loss:  2.294354200363159\n",
      "loss:  2.3008720874786377\n",
      "loss:  2.308385133743286\n",
      "loss:  2.3012590408325195\n",
      "loss:  2.292106866836548\n",
      "loss:  2.285461187362671\n",
      "loss:  2.2907345294952393\n",
      "loss:  2.2993223667144775\n",
      "loss:  2.2806508541107178\n",
      "loss:  2.2901713848114014\n",
      "loss:  2.2903223037719727\n",
      "loss:  2.2925503253936768\n",
      "loss:  2.295600175857544\n",
      "loss:  2.2935097217559814\n",
      "loss:  2.3105649948120117\n",
      "loss:  2.2740557193756104\n",
      "loss:  2.303849458694458\n",
      "loss:  2.3155875205993652\n",
      "loss:  2.290632486343384\n",
      "loss:  2.2721147537231445\n",
      "loss:  2.3068320751190186\n",
      "loss:  2.306914806365967\n",
      "loss:  2.3009097576141357\n",
      "loss:  2.280459403991699\n",
      "loss:  2.3081610202789307\n",
      "loss:  2.285501480102539\n",
      "loss:  2.3049848079681396\n",
      "loss:  2.2809364795684814\n",
      "loss:  2.3022115230560303\n",
      "loss:  2.270297050476074\n",
      "loss:  2.2939984798431396\n",
      "loss:  2.289551019668579\n",
      "loss:  2.2888388633728027\n",
      "loss:  2.305976152420044\n",
      "loss:  2.293292760848999\n",
      "loss:  2.30993914604187\n",
      "loss:  2.2982847690582275\n",
      "loss:  2.2950263023376465\n",
      "loss:  2.3205807209014893\n",
      "loss:  2.286233425140381\n",
      "loss:  2.2959516048431396\n",
      "loss:  2.3083441257476807\n",
      "loss:  2.291358470916748\n",
      "loss:  2.29304838180542\n",
      "loss:  2.2883660793304443\n",
      "loss:  2.3034794330596924\n",
      "loss:  2.2822303771972656\n",
      "loss:  2.2917580604553223\n",
      "loss:  2.28399395942688\n",
      "loss:  2.3179421424865723\n",
      "loss:  2.3024847507476807\n",
      "loss:  2.273736000061035\n",
      "loss:  2.2600841522216797\n",
      "loss:  2.2998266220092773\n",
      "loss:  2.2795960903167725\n",
      "loss:  2.2614874839782715\n",
      "loss:  2.291734218597412\n",
      "loss:  2.313502550125122\n",
      "loss:  2.305711030960083\n",
      "loss:  2.272857427597046\n",
      "loss:  2.301898241043091\n",
      "loss:  2.2843947410583496\n",
      "loss:  2.2795050144195557\n",
      "loss:  2.293980360031128\n",
      "loss:  2.296126127243042\n",
      "loss:  2.296487331390381\n",
      "loss:  2.2888519763946533\n",
      "loss:  2.2806193828582764\n",
      "loss:  2.3054182529449463\n",
      "loss:  2.293656826019287\n",
      "loss:  2.2825710773468018\n",
      "loss:  2.2844016551971436\n",
      "loss:  2.2812094688415527\n",
      "loss:  2.2831764221191406\n",
      "loss:  2.279367208480835\n",
      "loss:  2.3098368644714355\n",
      "loss:  2.3139548301696777\n",
      "loss:  2.293307065963745\n",
      "loss:  2.3019073009490967\n",
      "loss:  2.2947821617126465\n",
      "loss:  2.3003220558166504\n",
      "loss:  2.297496795654297\n",
      "loss:  2.302713394165039\n",
      "loss:  2.298358678817749\n",
      "loss:  2.3004465103149414\n",
      "loss:  2.279771327972412\n",
      "loss:  2.2848494052886963\n",
      "loss:  2.273183822631836\n",
      "loss:  2.291776180267334\n",
      "loss:  2.2828619480133057\n",
      "loss:  2.306525230407715\n",
      "loss:  2.263291358947754\n",
      "loss:  2.2781078815460205\n",
      "loss:  2.2692930698394775\n",
      "loss:  2.3155550956726074\n",
      "loss:  2.292827606201172\n",
      "loss:  2.2756004333496094\n",
      "loss:  2.2748498916625977\n",
      "loss:  2.284341812133789\n",
      "loss:  2.288588523864746\n",
      "loss:  2.2927372455596924\n",
      "loss:  2.2834558486938477\n",
      "loss:  2.2747340202331543\n",
      "loss:  2.2850513458251953\n",
      "loss:  2.2891879081726074\n",
      "loss:  2.2957985401153564\n",
      "loss:  2.28273868560791\n",
      "loss:  2.309236526489258\n",
      "loss:  2.283985137939453\n",
      "loss:  2.2808289527893066\n",
      "loss:  2.2826075553894043\n",
      "loss:  2.293010950088501\n",
      "loss:  2.2617297172546387\n",
      "loss:  2.3087003231048584\n",
      "loss:  2.288205623626709\n",
      "loss:  2.2942888736724854\n",
      "loss:  2.3004209995269775\n",
      "loss:  2.2958285808563232\n",
      "loss:  2.2808966636657715\n",
      "loss:  2.2591476440429688\n",
      "loss:  2.292083740234375\n",
      "loss:  2.299236297607422\n",
      "loss:  2.2947402000427246\n",
      "loss:  2.284942865371704\n",
      "loss:  2.3009378910064697\n",
      "loss:  2.2851452827453613\n",
      "loss:  2.2988007068634033\n",
      "loss:  2.2839341163635254\n",
      "loss:  2.280061960220337\n",
      "loss:  2.301424980163574\n",
      "loss:  2.2938201427459717\n",
      "loss:  2.2866299152374268\n",
      "loss:  2.276341438293457\n",
      "loss:  2.2779386043548584\n",
      "loss:  2.278682231903076\n",
      "loss:  2.2837460041046143\n",
      "loss:  2.2833383083343506\n",
      "loss:  2.272080421447754\n",
      "loss:  2.294395685195923\n",
      "loss:  2.272294282913208\n",
      "loss:  2.2813572883605957\n",
      "loss:  2.2851619720458984\n",
      "loss:  2.2673308849334717\n",
      "loss:  2.2842116355895996\n",
      "loss:  2.280702590942383\n",
      "loss:  2.2789084911346436\n",
      "loss:  2.2925009727478027\n",
      "loss:  2.2694034576416016\n",
      "loss:  2.283416509628296\n",
      "loss:  2.2861576080322266\n",
      "loss:  2.2821052074432373\n",
      "loss:  2.2892298698425293\n",
      "loss:  2.2569563388824463\n",
      "loss:  2.262714385986328\n",
      "loss:  2.2694315910339355\n",
      "loss:  2.2720887660980225\n",
      "loss:  2.276775598526001\n",
      "loss:  2.268077850341797\n",
      "loss:  2.2847132682800293\n",
      "loss:  2.2690887451171875\n",
      "loss:  2.2794792652130127\n",
      "loss:  2.2604024410247803\n",
      "loss:  2.268627643585205\n",
      "loss:  2.265658140182495\n",
      "loss:  2.270047664642334\n",
      "loss:  2.2546706199645996\n",
      "loss:  2.249720811843872\n",
      "loss:  2.2765090465545654\n",
      "loss:  2.253638982772827\n",
      "loss:  2.2604246139526367\n",
      "loss:  2.259101390838623\n",
      "loss:  2.2716798782348633\n",
      "loss:  2.256969690322876\n",
      "loss:  2.2513351440429688\n",
      "loss:  2.2643847465515137\n",
      "loss:  2.245296001434326\n",
      "loss:  2.2589197158813477\n",
      "loss:  2.2574663162231445\n",
      "loss:  2.243774890899658\n",
      "loss:  2.265589952468872\n",
      "loss:  2.2541685104370117\n",
      "loss:  2.2218096256256104\n",
      "loss:  2.2472870349884033\n",
      "loss:  2.2386767864227295\n",
      "loss:  2.2501273155212402\n",
      "loss:  2.2418813705444336\n",
      "loss:  2.252002000808716\n",
      "loss:  2.245313882827759\n",
      "loss:  2.2205114364624023\n",
      "loss:  2.237950563430786\n",
      "loss:  2.228921413421631\n",
      "loss:  2.2507684230804443\n",
      "loss:  2.234602451324463\n",
      "loss:  2.2199952602386475\n",
      "loss:  2.2537314891815186\n",
      "loss:  2.244260787963867\n",
      "loss:  2.256875514984131\n",
      "loss:  2.2247872352600098\n",
      "loss:  2.1995697021484375\n",
      "loss:  2.2238399982452393\n",
      "loss:  2.2194838523864746\n",
      "loss:  2.19374418258667\n",
      "loss:  2.214613437652588\n",
      "loss:  2.223440170288086\n",
      "loss:  2.201207399368286\n",
      "loss:  2.214000701904297\n",
      "loss:  2.2446768283843994\n",
      "loss:  2.194200038909912\n",
      "loss:  2.1921579837799072\n",
      "loss:  2.2044951915740967\n",
      "loss:  2.1623759269714355\n",
      "loss:  2.221193790435791\n",
      "loss:  2.1664068698883057\n",
      "loss:  2.2009572982788086\n",
      "loss:  2.240553855895996\n",
      "loss:  2.1567165851593018\n",
      "loss:  2.140070676803589\n",
      "loss:  2.190936803817749\n",
      "loss:  2.1630029678344727\n",
      "loss:  2.161357879638672\n",
      "loss:  2.137998342514038\n",
      "loss:  2.1822030544281006\n",
      "loss:  2.1928882598876953\n",
      "loss:  2.1605982780456543\n",
      "loss:  2.1190271377563477\n",
      "loss:  2.0688271522521973\n",
      "loss:  2.1770713329315186\n",
      "loss:  2.1544837951660156\n",
      "loss:  2.0721731185913086\n",
      "loss:  2.0786876678466797\n",
      "loss:  2.066673755645752\n",
      "loss:  2.0959677696228027\n",
      "loss:  2.0575451850891113\n",
      "loss:  2.0992848873138428\n",
      "loss:  2.02885103225708\n",
      "loss:  2.0204074382781982\n",
      "loss:  1.978636384010315\n",
      "loss:  1.870858073234558\n",
      "loss:  1.9802041053771973\n",
      "loss:  1.933618426322937\n",
      "loss:  1.968784213066101\n",
      "loss:  1.8886815309524536\n",
      "loss:  1.8249282836914062\n",
      "loss:  1.6961036920547485\n",
      "loss:  1.9826868772506714\n",
      "loss:  1.895984411239624\n",
      "loss:  1.8251458406448364\n",
      "loss:  1.8527716398239136\n",
      "loss:  1.8314547538757324\n",
      "loss:  1.8425153493881226\n",
      "loss:  1.7587640285491943\n",
      "loss:  1.689295768737793\n",
      "loss:  1.5216046571731567\n",
      "loss:  1.4568736553192139\n",
      "loss:  1.708855390548706\n",
      "loss:  1.5820311307907104\n",
      "loss:  1.7692713737487793\n",
      "loss:  1.7676459550857544\n",
      "loss:  1.6097967624664307\n",
      "loss:  1.4849703311920166\n",
      "loss:  1.6547207832336426\n",
      "loss:  1.5910037755966187\n",
      "loss:  1.5355021953582764\n",
      "loss:  1.5538599491119385\n",
      "loss:  1.2903516292572021\n",
      "loss:  1.3998504877090454\n",
      "loss:  1.5418370962142944\n",
      "loss:  1.219864010810852\n",
      "loss:  1.7917357683181763\n",
      "loss:  1.6310510635375977\n",
      "loss:  1.4015440940856934\n",
      "loss:  1.3391016721725464\n",
      "loss:  1.0355005264282227\n",
      "loss:  1.3339693546295166\n",
      "loss:  1.4083164930343628\n",
      "loss:  1.487411618232727\n",
      "loss:  2.6144962310791016\n",
      "loss:  1.706181287765503\n",
      "loss:  1.7605267763137817\n",
      "loss:  1.320394515991211\n",
      "loss:  1.444778323173523\n",
      "loss:  1.4823834896087646\n",
      "loss:  1.0363054275512695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.2398157119750977\n",
      "loss:  1.157535195350647\n",
      "loss:  1.1573939323425293\n",
      "loss:  1.4531832933425903\n",
      "loss:  1.4761362075805664\n",
      "loss:  1.4362030029296875\n",
      "loss:  1.5405813455581665\n",
      "loss:  1.3386307954788208\n",
      "loss:  0.9793723821640015\n",
      "loss:  1.253890872001648\n",
      "loss:  1.424901008605957\n",
      "loss:  1.2419023513793945\n",
      "loss:  1.1480393409729004\n",
      "loss:  1.6247925758361816\n",
      "loss:  1.4403247833251953\n",
      "loss:  1.2923094034194946\n",
      "loss:  1.1677192449569702\n",
      "loss:  0.9461109042167664\n",
      "loss:  0.9053545594215393\n",
      "loss:  1.3455042839050293\n",
      "loss:  1.5672012567520142\n",
      "loss:  1.4915506839752197\n",
      "loss:  1.0447895526885986\n",
      "loss:  1.0319924354553223\n",
      "loss:  1.1062811613082886\n",
      "loss:  0.7725600004196167\n",
      "loss:  1.271331787109375\n",
      "loss:  1.2717629671096802\n",
      "loss:  1.1483302116394043\n",
      "loss:  1.5338302850723267\n",
      "loss:  1.1409341096878052\n",
      "loss:  1.2556431293487549\n",
      "loss:  1.5101648569107056\n",
      "loss:  1.1231472492218018\n",
      "loss:  1.3897087574005127\n",
      "loss:  1.307887315750122\n",
      "loss:  1.2497624158859253\n",
      "loss:  1.1576497554779053\n",
      "loss:  1.6235464811325073\n",
      "loss:  1.656423807144165\n",
      "loss:  1.2758315801620483\n",
      "loss:  0.9515777826309204\n",
      "loss:  1.274449110031128\n",
      "loss:  1.2144981622695923\n",
      "loss:  0.9471729397773743\n",
      "loss:  0.8929488658905029\n",
      "loss:  0.9827297925949097\n",
      "loss:  1.0943061113357544\n",
      "loss:  1.0894821882247925\n",
      "loss:  1.1557769775390625\n",
      "loss:  1.1983649730682373\n",
      "loss:  1.2476096153259277\n",
      "loss:  1.0339016914367676\n",
      "loss:  0.8947705626487732\n",
      "loss:  0.6897255778312683\n",
      "loss:  1.0201789140701294\n",
      "loss:  1.0681087970733643\n",
      "loss:  0.8926748633384705\n",
      "loss:  1.2605770826339722\n",
      "loss:  1.2638956308364868\n",
      "loss:  1.6027979850769043\n",
      "loss:  1.806442379951477\n",
      "loss:  1.1902967691421509\n",
      "loss:  1.1734519004821777\n",
      "loss:  1.1062613725662231\n",
      "loss:  0.7586832046508789\n",
      "loss:  1.143058180809021\n",
      "loss:  1.043449878692627\n",
      "loss:  1.0563691854476929\n",
      "loss:  1.169571042060852\n",
      "loss:  1.2616329193115234\n",
      "loss:  1.203852891921997\n",
      "loss:  1.1737598180770874\n",
      "loss:  0.8792459964752197\n",
      "loss:  1.0866179466247559\n",
      "loss:  1.1499836444854736\n",
      "loss:  1.363548755645752\n",
      "loss:  1.1041043996810913\n",
      "loss:  0.9751220345497131\n",
      "loss:  1.1057244539260864\n",
      "loss:  0.7432202100753784\n",
      "loss:  1.135862946510315\n",
      "loss:  1.8730655908584595\n",
      "loss:  1.7091997861862183\n",
      "loss:  1.0897716283798218\n",
      "loss:  1.275285243988037\n",
      "loss:  1.0410364866256714\n",
      "loss:  0.8328971266746521\n",
      "loss:  0.7375077605247498\n",
      "loss:  1.0368235111236572\n",
      "loss:  1.1909551620483398\n",
      "loss:  0.9978618621826172\n",
      "loss:  0.8706868290901184\n",
      "loss:  0.9224888682365417\n",
      "loss:  1.0397268533706665\n",
      "loss:  0.760224461555481\n",
      "loss:  0.963512122631073\n",
      "loss:  0.9893323183059692\n",
      "loss:  0.5215548872947693\n",
      "loss:  1.5219969749450684\n",
      "loss:  1.2558763027191162\n",
      "loss:  0.9215137958526611\n",
      "loss:  0.6235352754592896\n",
      "loss:  1.1334989070892334\n",
      "loss:  1.0029457807540894\n",
      "loss:  1.0870822668075562\n",
      "loss:  0.628546953201294\n",
      "loss:  0.965251088142395\n",
      "loss:  1.404771089553833\n",
      "loss:  1.2331429719924927\n",
      "loss:  1.3217377662658691\n",
      "loss:  0.9705492854118347\n",
      "loss:  0.9677371978759766\n",
      "loss:  0.6045835614204407\n",
      "loss:  1.0698703527450562\n",
      "loss:  0.820076584815979\n",
      "loss:  0.7147858142852783\n",
      "loss:  1.3161262273788452\n",
      "loss:  1.8840018510818481\n",
      "loss:  1.6202948093414307\n",
      "loss:  1.2073063850402832\n",
      "loss:  1.2191736698150635\n",
      "loss:  0.8639978766441345\n",
      "loss:  1.0650036334991455\n",
      "loss:  1.8512418270111084\n",
      "loss:  1.4272561073303223\n",
      "loss:  1.7131950855255127\n",
      "loss:  1.3830903768539429\n",
      "loss:  1.0137100219726562\n",
      "loss:  1.9823648929595947\n",
      "loss:  1.592911958694458\n",
      "loss:  0.8363646268844604\n",
      "loss:  0.86285001039505\n",
      "loss:  0.8417222499847412\n",
      "loss:  1.1092885732650757\n",
      "loss:  1.371096134185791\n",
      "loss:  1.0366060733795166\n",
      "loss:  1.0493875741958618\n",
      "loss:  0.7495047450065613\n",
      "loss:  0.8810014724731445\n",
      "loss:  1.290310263633728\n",
      "loss:  0.931480884552002\n",
      "loss:  1.2832716703414917\n",
      "loss:  0.8192793130874634\n",
      "loss:  0.9648233652114868\n",
      "loss:  0.8069248795509338\n",
      "loss:  0.6466984748840332\n",
      "loss:  1.3470042943954468\n",
      "loss:  1.102126955986023\n",
      "loss:  1.8247982263565063\n",
      "loss:  1.0520563125610352\n",
      "loss:  1.4904769659042358\n",
      "loss:  0.9806098937988281\n",
      "loss:  0.799649715423584\n",
      "loss:  0.82347172498703\n",
      "loss:  0.868877649307251\n",
      "loss:  1.1445363759994507\n",
      "loss:  0.8934212327003479\n",
      "loss:  0.9895880818367004\n",
      "loss:  0.8375813961029053\n",
      "loss:  1.3713980913162231\n",
      "loss:  0.9955095052719116\n",
      "loss:  1.1222686767578125\n",
      "loss:  0.821721076965332\n",
      "loss:  0.7392314076423645\n",
      "loss:  1.0360307693481445\n",
      "loss:  1.1582839488983154\n",
      "loss:  1.2059259414672852\n",
      "loss:  0.8865363001823425\n",
      "loss:  1.0613116025924683\n",
      "loss:  0.9725320935249329\n",
      "loss:  0.7027738690376282\n",
      "loss:  0.6217591762542725\n",
      "loss:  0.6502166986465454\n",
      "loss:  1.4502735137939453\n",
      "loss:  1.176329255104065\n",
      "loss:  1.0866749286651611\n",
      "loss:  0.8739993572235107\n",
      "loss:  1.0693385601043701\n",
      "loss:  1.573607087135315\n",
      "loss:  1.3174610137939453\n",
      "loss:  1.1135181188583374\n",
      "loss:  0.49181973934173584\n",
      "loss:  0.7502145171165466\n",
      "loss:  0.8722900748252869\n",
      "loss:  0.9053456783294678\n",
      "loss:  0.9785347580909729\n",
      "loss:  1.762672781944275\n",
      "loss:  1.3848963975906372\n",
      "loss:  1.1089481115341187\n",
      "loss:  1.2411580085754395\n",
      "loss:  1.2619386911392212\n",
      "loss:  1.105143666267395\n",
      "loss:  1.0190256834030151\n",
      "loss:  0.7865667343139648\n",
      "loss:  0.8256294131278992\n",
      "loss:  1.1209416389465332\n",
      "loss:  1.184616208076477\n",
      "loss:  1.1208231449127197\n",
      "loss:  0.7729098200798035\n",
      "loss:  0.4065265655517578\n",
      "loss:  0.48866310715675354\n",
      "loss:  0.9166218042373657\n",
      "loss:  1.0943320989608765\n",
      "loss:  1.1805565357208252\n",
      "loss:  2.3349246978759766\n",
      "loss:  1.9721044301986694\n",
      "loss:  1.2364634275436401\n",
      "loss:  1.044485330581665\n",
      "loss:  1.092002034187317\n",
      "loss:  0.8935840129852295\n",
      "loss:  0.9403075575828552\n",
      "loss:  1.3493202924728394\n",
      "loss:  1.744423508644104\n",
      "loss:  1.3375321626663208\n",
      "loss:  1.7295860052108765\n",
      "loss:  1.5637425184249878\n",
      "loss:  1.4849447011947632\n",
      "loss:  1.3008365631103516\n",
      "loss:  0.9638307094573975\n",
      "loss:  1.2541505098342896\n",
      "loss:  0.8032659888267517\n",
      "loss:  0.8497431874275208\n",
      "loss:  0.9450680613517761\n",
      "loss:  1.234992265701294\n",
      "loss:  1.1725647449493408\n",
      "loss:  1.061171531677246\n",
      "loss:  1.0818592309951782\n",
      "loss:  0.6423106789588928\n",
      "loss:  0.5379605889320374\n",
      "loss:  1.2004667520523071\n",
      "loss:  0.6986996531486511\n",
      "loss:  0.7246100902557373\n",
      "loss:  1.3640151023864746\n",
      "loss:  1.7808374166488647\n",
      "loss:  1.3516227006912231\n",
      "loss:  2.7541050910949707\n",
      "loss:  1.99028742313385\n",
      "loss:  1.6726171970367432\n",
      "loss:  1.9904388189315796\n",
      "loss:  1.422593593597412\n",
      "loss:  1.8440450429916382\n",
      "loss:  1.517052412033081\n",
      "loss:  1.7476165294647217\n",
      "loss:  1.4893798828125\n",
      "loss:  1.4796055555343628\n",
      "loss:  2.2556569576263428\n",
      "loss:  1.8446873426437378\n",
      "loss:  2.1425843238830566\n",
      "loss:  2.293954849243164\n",
      "loss:  1.8718334436416626\n",
      "loss:  3.0923688411712646\n",
      "loss:  2.97890305519104\n",
      "loss:  2.3650734424591064\n",
      "loss:  2.327958345413208\n",
      "loss:  2.2676284313201904\n",
      "loss:  2.1925415992736816\n",
      "loss:  2.243947982788086\n",
      "loss:  2.0725209712982178\n",
      "loss:  2.2085800170898438\n",
      "loss:  2.171053647994995\n",
      "loss:  2.124847173690796\n",
      "loss:  1.8382450342178345\n",
      "loss:  2.0225300788879395\n",
      "loss:  3.615658760070801\n",
      "loss:  2.398395299911499\n",
      "loss:  2.313668966293335\n",
      "loss:  4.339369773864746\n",
      "loss:  16.392263412475586\n",
      "loss:  16.03033447265625\n",
      "loss:  427.9632568359375\n"
     ]
    }
   ],
   "source": [
    "# try multiple times (3-5 times) to get a good intuition and correctness\n",
    "model_lrfinder = FashionMnistNet()\n",
    "bs = 32\n",
    "loss_func = F.cross_entropy\n",
    "log_lrs, losses = find_lr(model_lrfinder, loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x121943748>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([10**x for x in log_lrs], losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Augmentation**\n",
    "\n",
    "Some canonical transforms:\n",
    "- Cropping image randomly\n",
    "- Rotation\n",
    "\n",
    "*You can use PyTorch's in-built transforms module*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch provides many methods for data augmentation\n",
    "# useful to understand data wrappers and data loaders\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "class FashionMnistDataset(Dataset):\n",
    "    \"Dataset class for Fashion Mnist data\"\n",
    "    def __init__(self, data_x, label_y, transforms=None):\n",
    "        self.data_x = data_x\n",
    "        self.label_y = label_y\n",
    "        self.transforms=transforms\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data_x[index]\n",
    "        y = self.label_y[index]\n",
    "        return self.transforms(x.view(1, 28, 28)), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(0.3),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dataset = FashionMnistDataset(x_train, y_train, transform)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Batch normalization](https://www.youtube.com/watch?v=tNIpEZLv_eg) **\n",
    "\n",
    "** [Reasons why batchnorm work are still being debated](https://twitter.com/dcpage3/status/1171867587417952260) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** [Transfer Learning](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Chapter including history of deep learning](https://www.deeplearningbook.org/contents/intro.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some resources for free GPUs (limited use but good enough for personal projects)\n",
    "[Google Colab](https://research.google.com/colaboratory/faq.html)\n",
    "\n",
    "[Paperspace](https://gradient.paperspace.com/free-gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Credits for images and gifs](https://me.me/i/tweaking-neural-net-parameter-mecenter-com-none-20844606)\n",
    "\n",
    "[](https://engmrk.com/lenet-5-a-classic-cnn-architecture/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/bin/bash: run.sh: command not found']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
